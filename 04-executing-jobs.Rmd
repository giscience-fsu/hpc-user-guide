# Submitting jobs

## `clustermq` setup

Every job submission is done via `clustermq::Q()` (either directly or via `drake`). 
See the setup instructions in the [clustermq](https://github.com/mschubert/clustermq/wiki) package on how to setup the package.

First, you need to set some options in your `.Rprofile` (on the master node):

```
options(
    clustermq.scheduler = "slurm",
    clustermq.template = "/path/to/file/below"
)
```

Note that you can have multiple `.Rprofile` files: One at the your home directory which is being used by your default R interpreter and one per project, at the root of your project directory.
This enables you to use custom R versions for each project

Once you did this, you should be able to run the [example](https://github.com/mschubert/clustermq) in the README of the _clustermq_ package.
It is a very simple example which finishes in a few seconds.
If it does not work, you either did something wrong or the nodes are busy.
Check with `sinfo` and `squeue`.
Otherwise see the [Troubleshooting](#troubleshooting) chapter.

```{block, type='rmdcaution'}
Be aware of setting `n_cpus` in the `template` argument of `clustermq::Q()` if your submitted job is parallelized!
If you submit a job that is parallelized without telling the scheduler, the scheduler will reserve 1 core for this job (because it thinks it is sequential) but in fact multiple processes will spawn. 
This will potentially affect all running processes on the server since the scheduler will accept more processing than it actually can take.
```

## From the master node

If you start jobs from the master node, you need to set the `.Rprofile` options given above.
Note that you can add any bash commands into the scripts between the `SBATCH` section and the final R call.
For example, one of my templates looks like this:

```sh
#!/bin/sh
#SBATCH --job-name={{ job_name }}
#SBATCH --partition=normal
#SBATCH --output={{ log_file | /dev/null }} # you can add .%a for array index
#SBATCH --error={{ log_file | /dev/null }}
#SBATCH --mem-per-cpu={{ memory | 2048 }}
#SBATCH --cpus-per-task={{ n_cpus }}
#SBATCH --array=1-{{ n_jobs }}

cd path/to/project

# make sure to set the desired R version
/opt/R/3.5.1/bin/R --no-save --no-restore -e 'clustermq:::worker("{{ master }}")'
```

Note: The `#` signs are no mistakes here, they are no "comment" signs in this context. 
The `SBATCH` commands will be executed here.

You can simply copy it and adjust it to your needs (set the right path to your project and specify the R version you want to use).

## From your local machine (via SSH) {#ssh-mode}

You can also use the servers right from your local machine without executing the job from the server explicitly (and hence without being logged in at all).

```{block, type='rmdcaution'}
Note that this mode is somewhat advanced and it is recommended to start jobs right from the master node as outlined above.
```

This works by sending requests to the scheduler via SSH.
Once the jobs are finished, the resulting objects are available on your local machine.
See the [ssh connector](https://github.com/mschubert/clustermq/wiki/Configuration#ssh-connector) section in the `clustermq` wiki for more detailed instructions.

To use this approach, you need to set the following `options()` in your projects `.Rprofile` on your **local** machine:

```r
options(
    clustermq.scheduler = "ssh",
    clustermq.ssh.host = "user@host", # use your user and host, obviously
    clustermq.ssh.log = "~/cmq_ssh.log" # log for easier debugging
)
```

Now every call to `clustermq::Q()` does the following:

1. An SSH connection to the server is established 
1. Your `~/.bashrc` file on the SMS is sourced and the R function call to `clustermq:::ssh_proxy()` is being executed.
1. This R call again uses the `options()` from your default `.Rprofile` which has a pointer to a `slurm_clustermq.tmpl` file which is then being called.

```r
clustermq:::ssh_proxy(ctl=54306, job=50570)
master ctl listening at: tcp://localhost:54306
forwarding local network from: tcp://gisc:8800
sent PROXY_UP to master ctl
received common data:
sent PROXY_READY to master ctl
received: PROXY_CMDqsys$submit_jobs(log_file = "/home/patrick/log.txt", n_jobs = 3)
```

3. The final job request is send to the scheduler which then runs the job / adds it to the queue (last line of the block above).

While this is very convenient as it enables processing on the server without even logging in, it currently has one downside:
The first call to `clustermq:::ssh_proxy()` takes the R configuration (= `.Rprofile`) specified in your default R interpreter on the SMS.
Here, you can specify the location of your `slurm_clustermq.tmpl` file which again loads your desired libraries via `spack` and changes the directory to your project directory so that your _packrat_ libraries are used (for example).
However, this means that you cannot work with multiple projects at the same with using the SSH approach as you can only point to one Slurm template file at the same time.

In summary the pipeline of the call is: `clustermq::Q()` (local) -> R (remote) -> `slurm_clustermq.tmpl` (specified in `.Rprofile`) (remote) -> SLURM -> processing

### Requirements 

#### Local

- The `clustermq` package needs to be installed
- `clustermq.scheduler` needs to be set to `"ssh"` in `options()`
- `clustermq.ssh.host` needs to be specified `options()`

#### Remote

On the server, you need to specify the location of the `slurm_clustermq.tmpl` file in `options()` in the `.Rprofile` of your default R interpreter:

```r
options(
  clustermq.scheduler = "slurm", 
  clustermq.template = "<path/to/file>"
)
```

Furthermore, all required R packages need to be installed on the server.

## Allocating resources

When submitting jobs via `clustermq::Q()`, it is important to tell the scheduler how many cores should be reserved via the `n_jobs` argument.
If omitted, the scheduler will plan with 1 core (= sequential execution) although your submitted code will eventually spawn multiple workers along the process.

There are two ways to specify this setting, depending on which approach you use:

1. via `clustermq::Q()` directly

Here you can directly set the `n_cpus` argument to the desired value. 
It will then be passed to the `slurm_clustermq.tmpl` file which has a line containing `#SBATCH --cpus-per-task{{ n_cpus }}`.
This tells the scheduler how many resources (here cpus) your job needs.
Note that the maximum number is `n_cpus = 32` as processes cannot be split across nodes.

2. via `drake::make()` (recommended)

In `drake::make()` you can set this option via the argument `template = list(n_cpus = X)`.
See section ["The resources column for transient workers"](https://ropenscilabs.github.io/drake-manual/hpc.html#advanced-options) in the drake manual.

```{block, type='rmdcaution'}
Please think upfront how many cpus your task requires. 
If you use something like `mclapply(cores = 20)` but set `n_cpus` in the Slurm template to 16, you will just get 16 cpus.
The remaining 4 workers are spawned but will always be in "waiting mode" as you only requested 16 cores.
```

Furthermore, if you want to use all ressources of a node and run into memory problems, try reducing the number of cpus.
Each node has 126 GB RAM available which gets split up to the nodes. 
If you scale down the number of cpus, you will have more memory/cpu available.

## Monitoring progress

When submitting jobs you can track its progress by specifying a `log_file` in the `clustermq::Q()` call, e.g. `clustermq::Q(template = list(log_file = path/to/file))`.

If your jobs are running on a node, you can SSH into it, e.g. `ssh c0`.
In the node you can take a look at the current load by using `htop`.
Note that you can only login if you have a running progress in a specific node.

## Summary

1. Decide which approach you want to use

- `drake::make(parallelism = "clustermq", template = list(n_cpus = X, log_file = Y))` (recommended!)
- `clustermq::Q(template = list(n_cpus = X, log_file = Y))`

Both can be used from your local machine (SSH) or from the master node (recommended!).

2. You need to have a Slurm template file in your project directory. This template needs to be referrended in your `.Rprofile` by `options(clustermq.template = "/path/to/file")`. The following settings are required:

- `R version`
- `--cpus-per-task`
- `--log_file`
- A `cd` command pointing to your project directory

## Best practice {#best-practice}

If you submit a job, you submit it to ONE node.
You can submit as many tasks as you want in separate R sessios, they will be processed as ranked in the queue of the scheduler.
However, submitting single jobs that finish quickly is tedious.
The same applies to the submission of multiple jobs in separate R sessions.

These two points are the reason why it is *highly* recommended to use `drake`.
By using `drake`, all of your intermediate R objects are "targets". 
You can specify any number of targets that should be build when calling `drake::make()`.
`drake` will take care that for each target as separate job is created which is then added to the Slurm queue.

For example, if you want to build three R objects named `object1`, `object2` and `object3` in parallel, one on each node:

`drake::make(plan, targets = c("object1", "object2", "object3"), jobs = 3, template = list(n_cpus = 16, log_file = "/path/to/log.txt"))`

- Creates three jobs which are added to the Slurm queue
- Each jobs requires 16 cores
- The job is finished once all targets have been built.

After they are finished, they will be marked as "built" and added to a cache directory by `drake`, so you cannot accidentally rebuild them (unless you change somehting substantial in the code).

If you follow this practive, the only thing you need to execute on the cluster will be this call (after you initialized the config by sourcing your `drake.R` file).


