# Submitting jobs

## `clustermq` setup

Every job submission is done via `clustermq::Q()` (either directly or via `drake`). 
See the setup instructions in the [clustermq](https://mschubert.github.io/clustermq/) package on how to setup the package.

First, you need to set some options in your `.Rprofile` (on the master node):

```
options(
    clustermq.scheduler = "slurm",
    clustermq.template = "</path/to/file/below>"
)
```

`clustermq.template` should point to a `clustermq.template` file. 
Take a look at the specific instructions in the respective [package vignette](https://mschubert.github.io/clustermq/articles/userguide.html#slurm).

Note that you can have multiple `.Rprofile` files on your system:

1. Your default R interpreter will use the `.Rprofile` found in the home directory (`~/`).
1. But you can also save an `.Rprofile` file in the root directory of a (RStudio) project. 

This way you can use customized `.Rprofile` files specifically written for a certain project.

Now you should be able to run the [example](https://github.com/mschubert/clustermq) at the top of the `README` of the _clustermq_ package.
It is a very simple example which finishes in a few seconds.
If it does not work, you either did something wrong or the nodes are busy.
Check with `sinfo` and `squeue`.
Otherwise see the [Troubleshooting](#troubleshooting) chapter.

```{block, type='rmdcaution'}
Be aware of setting `n_cpus` in the `template` argument of `clustermq::Q()` if your submitted job is parallelized!
If you submit a job that is parallelized without telling the scheduler, the scheduler will reserve 1 core for this job (because it thinks it is sequential) but in fact multiple processes will spawn. 
This will potentially affect all running processes on the server since the scheduler will accept more processing than it actually can take.
```

## The scheduler template

To successfully submit jobs to the scheduler, you need to set the `.Rprofile` options given above.
Note that you can add any bash commands into the scripts between the `SBATCH` section and the final R call.
For example, one of my templates looks like this:

```sh
#!/bin/sh
#SBATCH --job-name={{ job_name }}
#SBATCH --partition=normal
#SBATCH --output={{ log_file | /dev/null }} # you can add .%a for array index
#SBATCH --error={{ log_file | /dev/null }}
#SBATCH --cpus-per-task={{ n_cpus }}
#SBATCH --mem={{ memory }}
#SBATCH --array=1-{{ n_jobs }}

cd path/to/project

spack load r@<version>

# add more spack libraries here

CMQ_AUTH={{ auth }} R --no-save --no-restore -e 'clustermq:::worker("{{ master }}")'
```

Note: The `#` signs are no mistakes here, they are no "comment" signs in this context. 
The `SBATCH` commands will be executed here.

You can simply copy it and adjust it to your needs (set the right path to your project and specify the R version you want to use).

## Allocating resources

There are two approaches/packages you can use: 

- `drake` (recommended)

- `clustermq`

```{r eval = FALSE}
drake::make(parallelism = "clustermq", n_jobs = 1, 
  template = list(n_cpus = <X>, log_file = <Y>, memory = <Z>))
```

```{r eval = FALSE}
clustermq::Q(template = list(n_cpus = <X>, log_file = <Y>, memory = <Z>))
```

(The individual components of these calls are explained in more detail below.
Note that `drake` uses `clustermq` under the hood. 
Notations like `<X>` are meant to be read as placeholders, meaning you need to replace them with valid content.)

When submitting jobs via `clustermq::Q()`, it is important to tell the scheduler how many cores and memory should be reserved for you.
This step is very important.
If you specify less cores than you actually use in your script (e.g. by internal parallelization), the scheduler will plan with X cores although your submitted code will spawn Y processes in the background.
This might overload the node and eventually cause your script (and more importantly) the processes of others to crash.

There are two ways to specify these settings, depending on which approach you use:

1. via `clustermq::Q()` directly

Pass the values via argument `template` like `template = list(n_cpus = X, memory = Y)`.
It will then be passed to the `clustermq.template` file (frequently named `slurm_clustermq.tmpl`) which contains following lines:

```sh
#SBATCH --cpus-per-task{{ n_cpus }}
#SBATCH --mem={{ memory }}
```

This tells the scheduler how many resources (here cpus) your job needs.
Note that the maximum values are `n_cpus = 32` and `memory = 126000` per node.

2. via `drake::make()` (recommended)

Again, set the options via argument `template = list(n_cpus = X, memory = Y)`.
See section ["The resources column for transient workers"](https://ropenscilabs.github.io/drake-manual/hpc.html#advanced-options) in the drake manual.

```{block, type='rmdcaution'}
Please think upfront how many cpus and memory your task requires. 
The following two examples show you the implications of wrong specifications.
```

```{block, type='rmdcaution'}
`mclapply(cores = 20)` > `n_cpus = 16`

In this case, four workers will always be in "waiting mode" since only 16 cpus can be used by your resource request. 
This slows down your parallelization but does no harm to other users. 
```

```{block, type='rmdcaution'}
`mclapply(cores = 11)` < `n_cpus = 16`

In this case, you reserve 16 CPUs from the machine but only use 11 at most. 
This blocks five CPUs of the machine for no reason potentially causing other people to be added to the queue rather than getting their job processed immediately.
```

Furthermore, if you want to use all resources of a node and run into memory problems, try reducing the number of CPUs (if you already increased the memory to its maximum).
If you scale down the number of CPUs, you will have more memory/cpu available.

## Monitoring progress

When submitting jobs you can track its progress by specifying a `log_file` in the `clustermq::Q()` call, e.g. `clustermq::Q(template = list(log_file = path/to/file))`.

For `drake`, the equivalent is to specify `console_log_file()` in either `make()` or `drake_config()`.

If your jobs are running on a node, you can SSH into the node, e.g. `ssh c0`.
In the node you can take a look at the current load by using `htop`.
Note that you can only login if you have a running progress in a specific node.

Another option is to take a look [Ganglia](http://141.35.158.107/ganglia/?r=hour&cs=&ce=&m=load_one&s=by+name&c=OpenHPC&tab=m&vn=&hide-hf=false) to see the load of the HPC.

## Summary

1. Set up your `.Rprofile` with `options(clustermq.template = "/path/to/file")`.
  The `clustermq.template` should point to a slurm template file in your home or project directory.

1. Decide which approach you want to use:

- `drake::make(parallelism = "clustermq", n_jobs = 1, template = list(n_cpus = X, log_file = Y, memory = Z))` (recommended!)
- `clustermq::Q(template = list(n_cpus = X, log_file = Y, memory = Z))`

2. You need to have a Slurm template file in your project directory. This template needs to be linked in your `.Rprofile` with `options(clustermq.template = "/path/to/file")`.

## Best practice {#best-practice}

If you submit a job, you submit it to ONE node.
You can submit as many tasks as you want in separate R sessions, they will be processed as ranked in the queue of the scheduler.
However, submitting single jobs that finish quickly is tedious.
The same applies to the submission of multiple jobs in separate R sessions.

These two points are the reason why it is *highly* recommended to use `drake`.
By using `drake`, all of your intermediate R objects are "targets". 
You can specify any number of targets that should be build when calling `drake::make()`.
`drake` will take care that for each target as separate job is created which is then added to the Slurm queue.

For example, if you want to build three R objects named `object1`, `object2` and `object3` in parallel, one on each node:

```r
drake::make(plan, targets = c("object1", "object2", "object3"), jobs = 3, 
            template = list(n_cpus = 16, memory = 320000, 
            log_file = "/path/to/log.txt"))
```

- Creates three jobs which are added to the Slurm queue
- Each jobs requires 16 cores
- Each job needs 32 GB of memory
- The call is finished once all jobs have finished.

After they are done, they will be marked as "built" and added to a cache directory by `drake`, so you cannot accidentally rebuild them (unless you change something substantial in the code).

If you follow this practice, the only thing you need to execute on the cluster will be this call (after you initialized the config by sourcing your `drake.R` file).
