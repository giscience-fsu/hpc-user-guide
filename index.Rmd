--- 
title: "HPC User Guide"
author: "Patrick Schratz, Jannes Muenchow"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
# bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
# github-repo: rstudio/bookdown-demo
# description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Introduction {#intro}

This is a user manual for the High-Performance-Server (HPC) of the GIScience chair at the Friedrich-Schiller-University Jena.

Its main purpose is to do R processing and its setup is tailored towards this.
Nevertheless, of course also any job can be submitted to the HPC.
However, the following manual will focus on getting R code running.

The big advantage of a HPC is that users can submit jobs to ONE remote server which then distributes the work across multiple machines in the background.
Incoming processing requests (jobs) are handled by the installed scheduler (SLURM), taking away the work of scheduling the job and the fear of clashing with other users.
The status of the job request can be queried easily.
Administration is simplified by provisioning all compute nodes with the same virtual image.
This way, update tasks are reduced and differences between the machines are avoided.
Administration is further simplified by using the [Spack]() package manager as this application allows for version-agnostic library installations on the user level without the need of requesting and waiting for custom installation to the admin.

The $HOME directory of every used is shared across all nodes, avoiding the need to keep data and scripts in sync across multiple machines.

**Before you start**, please add a SSH key to your account to be able to login without password.
Your password will consist of 20 characters and you will not remember it.
See [this](https://help.github.com/articles/connecting-to-github-with-ssh/) guide if you have never worked with SSH keys before.
If you have, you can also use `ssh-copy-id <username>@141.35.158.107` to copy your key to the server.
Afterwards you should be able to login via `ssh <username>@141.35.158.107` without being prompted for your password.
A working SSH setup is also required if you want to use the [SSH way](#ssh-mode) to schedule jobs.

## Web Address

* edi.geogr.uni-jena.de
* IP: 141.35.158.107

## Hardware

The cluster consists of multiple compute node groups.

The newest group of compute nodes has the following specifications:

- CPU: AMD Threadripper 2950X, 16-core, 3.5 GHz - 4.4 GHz
- RAM: 126 GB DDR4 
- 1 TB M2 SSD
- Number of nodes: 3

The master node also belongs to this group.
Due to management overhead, only 8 cores (16 CPUs) can be used.

Soon there will be a second group of computes nodes added (the old machines that we used up until now).

## Software

The HPC was built following the installation guide provided by the [Open HPC](https://openhpc.community/) community (using the "Warewulf + Slurm" edition).  
It is currently running on a CentOS 7.6 operating system using the CentOS main-line Kernel (currently at version 5.0).  
The scheduler that is used for queuing processing job requests is [SLURM](https://slurm.schedmd.com/).
Load monitoring is performed via [Ganglia](http://ganglia.sourceforge.net/). 
A live view is accessible [here](http://edi.geogr.uni-jena.de/ganglia/?r=hour&cs=&ce=&m=load_one&s=by+name&c=&tab=m&vn=&hide-hf=false).  
[Spack](https://spack.io/) is used as the package manager.
More detailed instructions on the scheduler and the package manager can be found in their respective chapters.

## Data storage

Currently all data is written to the 1 TB SSD of the master node.
As soon as the new 54 TB data server is up and running it will be used for the `/home` partition.

## Accessing files from your local computer

It is recommended to mount the server via `sshfs` to your local machine.
Transfer speed ranges between 50 - 100 Mbit/s when you're in the office so you should be able to access files without a delay.
Accessing files from outside will be slower.

### Unix

For Unix system, this can be easily done from the command line via

```sh
sudo sshfs -o reconnect,idmap=user,transform_symlinks,identityFile=~/.ssh/id_rsa,allow_other,cache=yes,kernel_cache,compression=no,default_permissions,uid=1000,gid=100,umask=0 <username>@141.35.158.107:/ <local-directory>
```

The mount process is passwordless if you do it via SSH (i.e. via your `~/.ssh/id_rsa` key).
Note that the mount is actually performed by the root user, so you need to copy your SSH key to the root user: `cp ~/.ssh/id_rsa /root/.ssh/id_rsa`.

For convenience you can create an executable script that performs this action every time you need it.

```{block, type='rmdcaution'}
Auto-mount during boot via `fstab` is not recommended since sometimes the network is not yet up in the moment when the mount is executed.
This applies especially if you are not in the office but accessing the server from outside.
```

### Windows

Please install [sshfs-win](https://github.com/billziss-gh/sshfs-win) and follow the instructions.
