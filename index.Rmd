--- 
title: "HPC User Guide"
author: "Patrick Schratz"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
# bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
# github-repo: rstudio/bookdown-demo
# description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Introduction {#intro}

This is a user manual for the High-Performance-Server (HPC) of the GIScience chair at the Friedrich-Schiller-University Jena.

Its main purpose is to do R processing and its setup is tailored towards this.
Nevertheless, of course also any job can be submitted to the HPC.
However, the following manual will focus on getting R code running.

The big advantage of a HPC is that users can submit jobs to ONE remote server which then distributes the work across multiple machines in the background.
Incoming processing requests (jobs) are handled by the installed scheduler (SLURM), taking away the work of scheduling the job and the fear of clashing with other users.
The status of the job request can be queried easily.
Administration is simpliefied by provisioning all compute nodes with the same virtual image.
This way, update tasks are reduced and differences between the machines are avoided.
Administration is further simplified by using the [Spack]() package manager as this application allows for version-agnostis library installations on the user level without the need of requesting and waiting for custom installation to the admin.

The $HOME directory of every used is shared across all nodes, avoiding the need to keep data and scripts in sync across multiple machines.

**Before you start**, please add a SSH key to your account to be able to login without password.
Your password will consist of 20 characters and you will not remember it.
See [this](https://help.github.com/articles/connecting-to-github-with-ssh/) guide if you have never worked with SSH keys before.
If you have, you can also use `ssh-copy-id <username>@10.35.158.19` to copy your key to the server.
Afterwards you should be able to login via `ssh <username>@10.35.158.19` without being prompted for your password.
A working SSH setup is also requiered if you want to use the [SSH way](#ssh-mode) to schedule jobs.

## Web Address

* gisc.ads.uni-jena.de (short for GIS cluster)
* IP: 10.35.158.19

## Hardware

Each node of the HPC has the following hardware:

- CPU: Threadripper 2950X, 16-core, 3.5 GHz - 4.4 GHz
- RAM: 126 GB DDR4 
- 1 TB M2 SSD

Currently, three nodes are available for processing. 
The master node (which will be referred to as the _system management server_ (SMS) from here on) is currently not used as it is hosting the _Rstudio Server_ application.
Depending on the load, we might also add it as a computing node in the future.

## Software

The HPC was built following the installation guide provided by the [Open HPC](https://openhpc.community/) community (using the "Warewulf + Slurm" edition).
It is currently running on a CentOS 7.6 operating system using the CentOS main-line Kernel (currently at version 4.20).
The scheduler that is used for queuing processing job requests is [SLURM](https://slurm.schedmd.com/).
Load monitoring is performed via [Ganglia](http://ganglia.sourceforge.net/). 
A live view is accessable [here](http://10.35.158.19/ganglia/?r=hour&cs=&ce=&m=load_one&s=by+name&c=&tab=m&vn=&hide-hf=false).
[Spack][https://spack.io/] is used as the package manager.
More detailed instructions on the scheduler and the package manager can be found in their respective chapters.

## Data storage

The data storage mount will be located at `/mnt/data` (via NFS).
You are free to choose whether the server will be your primary working location holding all your scripts (and data) or whether you want to keep your scripts locally at your machine.
If you use the [SSH way](#ssh-mode) to execute jobs on the HPC, you do not even need to login to the server to be able to run your scripts.
