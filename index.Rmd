--- 
title: "HPC User Guide"
author: "Patrick Schratz"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
# bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
# github-repo: rstudio/bookdown-demo
# description: "This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook."
---

# Introduction {#intro}

This is a user manual for the High-Performance-Server (HPC) of the GIScience chair at the Friedrich-Schiller-University Jena.

Its main purpose is to do R processing and its setup is tailored towards this.
Nevertheless, of course also any job can be submitted to the HPC.
However, the following manual will focus on getting R code running.

The big advantage of a HPC is that users can submit jobs to ONE remote server which then distributes the work across multiple machines in the background.
Incoming processing requests (jobs) are handled by the installed scheduler (SLURM), taking away the work of scheduling the job and the fear of clashing with other users.
The status of the job request can be queried easily.
Administration is simpliefied by provisioning all compute nodes with the same virtual image.
This way, update tasks are reduced and differences between the machines are avoided.
Administration is further simplified by using the [Spack]() package manager as this application allows for version-agnostis library installations on the user level without the need of requesting and waiting for custom installation to the admin.

The $HOME directory of every used is shared across all nodes, avoiding the need to keep data and scripts in sync across multiple machines.

**Before you start**, please add a SSH key to your account to be able to login without password.
Your password will consist of 20 characters and you will not remember it.
See [this](https://help.github.com/articles/connecting-to-github-with-ssh/) guide if you have never worked with SSH keys before.
If you have, you can also use `ssh-copy-id <username>@141.35.158.107` to copy your key to the server.
Afterwards you should be able to login via `ssh <username>@141.35.158.107` without being prompted for your password.
A working SSH setup is also requiered if you want to use the [SSH way](#ssh-mode) to schedule jobs.

## Web Address

* edi.geogr.uni-jena.de
* IP: 141.35.158.107

## Hardware

The cluster consists of multiple compute node groups.

The newest group of compute nodes has the following specifications:

- CPU: AMD Threadripper 2950X, 16-core, 3.5 GHz - 4.4 GHz
- RAM: 126 GB DDR4 
- 1 TB M2 SSD
- Number of nodes: 3

The master node also belongs to this group.
Due to management overhead, only 8 cores (16 CPUs) can be used.

The second group of computes has the following specs:

- CPU: AMD Opteron 6172
- RAM: 128 GB DDR3 (1.3 Mhz)
- Number of nodes: 4

Group X is about 3.5 times faster than group Y. If no jobs are running, new jobs will always be started on group X.

TODO: Name node groups

## Software

The HPC was built following the installation guide provided by the [Open HPC](https://openhpc.community/) community (using the "Warewulf + Slurm" edition).  
It is currently running on a CentOS 7.6 operating system using the CentOS main-line Kernel (currently at version 5.0).  
The scheduler that is used for queuing processing job requests is [SLURM](https://slurm.schedmd.com/).
Load monitoring is performed via [Ganglia](http://ganglia.sourceforge.net/). 
A live view is accessable [here](http://edi.geogr.uni-jena.de/ganglia/?r=hour&cs=&ce=&m=load_one&s=by+name&c=&tab=m&vn=&hide-hf=false).  
[Spack](https://spack.io/) is used as the package manager.
More detailed instructions on the scheduler and the package manager can be found in their respective chapters.

## Data storage

Currently all data is written to the 1 TB SSD of the master node.
The plan is to use our 54 TB data server for the `/home` directories so that we have enough space for everyone.
