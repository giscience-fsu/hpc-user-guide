[{"path":"index.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Welcome user manual High-Performance-Server (HPC) GIScience group (University Jena).tailored towards R processing.\ndocument describes get started submit jobs cluster.short introduction HPCsThe big advantage HPC users can submit jobs ONE machine distributes work across multiple machines background.\nIncoming processing requests (jobs) handled scheduler (SLURM), taking away work queuing job potential issue clashing jobs users.Administration simplified provisioning computing nodes virtual image.\nway, maintenance tasks reduced differences machines avoided.\nAdministration simplified using Spack package manager application allows version-agnostic environment module installations.$HOME directory every user shared across nodes, avoiding need keep data scripts sync across multiple machines.start:Working Linux server naturally requires certain amount familiarity UNIX command-line shells text editors.\ndozens Linux online tutorials help get started.1\ncourse, also great books use Linux Shotts (2012), Sobell (2010) Ward (2015) freely available.\nstill get stuck, Google might help .Working Linux server naturally requires certain amount familiarity UNIX command-line shells text editors.\ndozens Linux online tutorials help get started.1\ncourse, also great books use Linux Shotts (2012), Sobell (2010) Ward (2015) freely available.\nstill get stuck, Google might help .Please add SSH key pair account able log server without type password.\nespecially useful since password consist many letters numbers (> 10) want memorize.\nSee guide never worked SSH keys .\nalready use SSH key pair machine, can use ssh-copy-id <username>@141.35.158.107 copy key server.\nAfterwards able login via ssh <username>@141.35.158.107 without prompted password.Please add SSH key pair account able log server without type password.\nespecially useful since password consist many letters numbers (> 10) want memorize.\nSee guide never worked SSH keys .\nalready use SSH key pair machine, can use ssh-copy-id <username>@141.35.158.107 copy key server.\nAfterwards able login via ssh <username>@141.35.158.107 without prompted password.","code":""},{"path":"index.html","id":"web-address","chapter":"1 Introduction","heading":"1.1 Web Address","text":"https://edi.geogr.uni-jena.deIP: 10.232.16.28","code":""},{"path":"index.html","id":"hardware","chapter":"1 Introduction","heading":"1.2 Hardware","text":"cluster consists following machines:Group “threadripper”:CPU: AMD Threadripper 2950X, 16-core, Hyperthreading support, 3.5 GHz - 4.4 GHzRAM: 126 GB DDR4Number nodes: 4 c[0-2] (+ frontend)“frontend” operating 12 cores 100 GB RAMGroup “opteron”:CPU: AMD Opteron 6172, 48 cores, Hyperthreading, 2.1 GHzRAM: 252 GB DDR3 (c5 comes 130 GB RAM)Number nodes:\n2 (c[3-4])\n1 (c5)\n2 (c[3-4])1 (c5)groups reflected scheduler via “partition” setting.Group “threadripper” 3.5x faster group “opteron”.","code":""},{"path":"index.html","id":"software","chapter":"1 Introduction","heading":"1.3 Software","text":"HPC built following installation guide provided Open HPC community (using “Warewulf + Slurm” edition) operates CentOS 7 base.\nscheduler used queuing processing job requests SLURM.\nLoad monitoring performed via Ganglia.\nlive view accessible .\nSpack used package manager.\ndetailed instructions scheduler package manager can found respective chapters.","code":""},{"path":"index.html","id":"data-storage","chapter":"1 Introduction","heading":"1.4 Data storage","text":"mars data server mounted /home stores data.\nCurrently capacity 20 TB users combined.\nData can stored directly /home directory.","code":""},{"path":"index.html","id":"accessing-files-from-your-local-computer","chapter":"1 Introduction","heading":"1.5 Accessing files from your local computer","text":"recommended mount server via sshfs local machine.\nTransfer speed ranges 50 - 100 Mbit/s ’re office able access files without delay.\nAccessing files outside slower.really run trouble transfer speed, directly connect mars server.Otherwise, route follows: <local> (sshfs) -> edi (nfs) -> mars","code":""},{"path":"index.html","id":"unix","chapter":"1 Introduction","heading":"1.5.1 Unix","text":"Unix system, following command can usedThe mount process passwordless via SSH (.e. via ~/.ssh/id_rsa key).\nNote mount actually performed root user, need copy SSH key root user: cp ~/.ssh/id_rsa /root/.ssh/id_rsa.convenience can create executable script performs action every time need .\nAuto-mount boot via fstab recommended since sometimes network yet mount executed. applies especially office accessing server outside.\n","code":"sudo sshfs -o reconnect,idmap=user,transform_symlinks,identityFile=~/.ssh/id_rsa,allow_other,cache=yes,kernel_cache,compression=no,default_permissions,uid=1000,gid=100,umask=0 <username>@141.35.158.107:/ <local-directory>"},{"path":"index.html","id":"windows","chapter":"1 Introduction","heading":"1.5.2 Windows","text":"Please install sshfs-win follow instructions.","code":""},{"path":"index.html","id":"accessing-the-hpc-remotely-vpn","chapter":"1 Introduction","heading":"1.6 Accessing the HPC remotely (VPN)","text":"VPN connection required access HPC remotely.\nFSU uses Cisco Anyconnect protocol VPN purposes recommends use “Cisco Anyconnect Client”.GUI requires manual connect/disconnect actions.\naddition, Cisco Anyconnect client known slow laggy (sorry references , just personal experience).\nopen-source alternative openconnect much better job - comes powerful CLI implementation.\nGUI preferred, look (looks like Windows way around GUI?).InstallationmacOS: brew install openconnectWindows: choco install openconnect-gui downloading GUI directlyUsageOn UNIX-based system:Start: echo <PASSWORD> | sudo openconnect --user=<USER>@uni-jena.de --passwd--stdin --background vpn.uni-jena.de\nStop: sudo killall -SIGINT openconnectOn Windows: Sorry, using Windows - feel free add info .","code":""},{"path":"index.html","id":"sharing-of-results","chapter":"1 Introduction","heading":"1.7 Sharing of Results","text":"","code":""},{"path":"index.html","id":"workflowr","chapter":"1 Introduction","heading":"1.7.1 workflowr","text":"Take look R package workflowr allows automatic deployment R Notebooks.","code":""},{"path":"index.html","id":"local-web-server","chapter":"1 Introduction","heading":"1.7.2 Local web server","text":"Jupiter server (141.35.159.87) set web-server.\nmeans able render HTML contents list files directory listing.Jupiter mounted cluster /mnt/nfs/jupiter.\nuse functionality, simply need copy desired files /home/www/<folder> Jupiter.\n, please follow steps:Request account Jupiter AndreasRequest account Jupiter AndreasLogin Jupiter create desired folders.\npublic URL relates follows local directory:\nhttps://jupiter.geogr.uni-jena.de/<folder> -> /home/www/<folder>Login Jupiter create desired folders.\npublic URL relates follows local directory:https://jupiter.geogr.uni-jena.de/<folder> -> /home/www/<folder>Copy files cluster desired location Jupiter using following code\nrsync -rlptDvzog --chown=www-data:www-data --fake-super \\\n  /mnt/nfs/jupiter/<path---file> \\\n  -e ssh <username>@jupiter.geogr.uni-jena.de:/home/www/<path--directory>\ncode copy files changed compared last execution can safely automate call workflow.Copy files cluster desired location Jupiter using following codeThis code copy files changed compared last execution can safely automate call workflow.Since contents available everyone, want add README.md file directory listing well Impressum.\nEven though one usually knows URL default, careful sharing sensible files.See https://jupiter.geogr.uni-jena.de/life-healthy-forest/ example.\ncan request theme Andreas.","code":"rsync -rlptDvzog --chown=www-data:www-data --fake-super \\\n  /mnt/nfs/jupiter/<path-to-your-file> \\\n  -e ssh <username>@jupiter.geogr.uni-jena.de:/home/www/<path-to-directory>"},{"path":"libraries.html","id":"libraries","chapter":"2 Libraries and Environment Modules","heading":"2 Libraries and Environment Modules","text":"","code":""},{"path":"libraries.html","id":"introduction","chapter":"2 Libraries and Environment Modules","heading":"2.1 Introduction","text":"\nPlease note RStudio Workbench decoupled Slurm. Everything run nodes via Slurm requires respective env modules loaded Slurm template file whereas run something RStudio server, libraries installed underlying Ubuntu 20.04 container used.\nfollowing part deals Spack configuration side everything container just work (respect R).system relevant libraries installed via Spack.\nHowever, user need worry since loading everything via “environment modules”.\nAvailable modules can queried using modules avail.setup reasons, please put following top ~/.bashrc fileHere snapshot:","code":"export SPACK_ROOT=/opt/spack\n. $SPACK_ROOT/share/spack/setup-env.sh------------------------- /opt/spack/share/spack/modules/linux-centos7-x86_64 --------------------------\n   byobu-5.127-gcc-9.2.0-by2qc2g          (L)    python-3.7.4-gcc-9.2.0-nbjbfzi     (L)\n   ccache-3.3.4-gcc-9.2.0-v3xzqqh         (L)    r-3.5.2-gcc-9.2.0-oxo76vo\n   curl-7.63.0-gcc-9.2.0-cq4w37y          (L)    r-3.6.1-gcc-9.2.0-j25wr6z\n   fish-3.0.0-gcc-9.2.0-gdyab6r      "},{"path":"libraries.html","id":"loading-modules","chapter":"2 Libraries and Environment Modules","heading":"2.2 Loading modules","text":"Modules can loaded via module load <module>.First, need load C compiler programs depend .\nmodule one uses gcc-4.8.5.\nMake sure also put one ~/.bashrc file!Now can (example) load GDAL:can check worked via","code":"module load gcc-9.2.0-gcc-4.8.5-wqdecm4module load gdal-2.4.2-gcc-9.2.0-henhg26gdalinfo --version"},{"path":"libraries.html","id":"checking-loaded-modules","chapter":"2 Libraries and Environment Modules","heading":"2.3 Checking loaded modules","text":"","code":"module list"},{"path":"libraries.html","id":"default-modules","chapter":"2 Libraries and Environment Modules","heading":"2.4 Default modules","text":"useful modules loaded default logging server.\nSimply add ~/.bashrc.running R spatial analysis, following required:Furthermore, recommend following ones (don’t forget append version compiler):might wonder shouldn’t add R version list? Good question!can load R module default - just note effect RStudio Workbench.\nSlurm project configurations specify R version use within template rely default set somewhere.","code":"module load \\\n  gcc-9.2.0-gcc-4.8.5-wqdecm4 \\\n  gdal-2.4.2-gcc-9.2.0-henhg26 \\\n  geos-3.7.2-gcc-9.2.0-jxq7o2n \\\n  git-2.21.0-gcc-9.2.0-o45v5mj \\\n  proj-5.2.0-gcc-9.2.0-5t4r5t4 \\\n  udunits2-2.2.24-gcc-9.2.0-hupsw3n \\\n  zlib-1.2.11-gcc-9.2.0-j3zdmzqbyobu-5.127-gcc-9.2.0-x5e5mnz\ngit-2.21.0-gcc-9.2.0-hhkbkhg\nccache-3.3.4-gcc-9.2.0-7klqklq\npython-3.7.4-gcc-9.2.0-nbjbfzi\ncurl-7.63.0-gcc-9.2.0-cq4w37y"},{"path":"libraries.html","id":"r","chapter":"2 Libraries and Environment Modules","heading":"2.5 R","text":"","code":""},{"path":"libraries.html","id":"slurm","chapter":"2 Libraries and Environment Modules","heading":"2.5.1 SLURM","text":"versions v3.4.4 upwards installed.load specific R version Slurm usage, load specific environment moduleand execute R.might find following aliases helpful can put ~/.bashrc:enables launch specific R version using set command, e.g. r360 launch R v3.6.0.BLAS/LAPACKUnfortunately use external BLAS/LAPACK like openblas due different architectures nodes.\ncompiling R use external BLAS/LAPACK, openblas tailors arch main node (threadripper) specific processes (e.g. function mgcv) package segfaultThe use R internal BLAS/LAPACK comes substantial decrease speed numerical applications.","code":"# loads R v3.5.2\nmodule load r-3.5.2-gcc-9.2.0-oxo76voalias r344=\"export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.4.4-hobcfrryiwu34etqvf4khv427wwkxdf5/rlib/R && radian\"\nalias r350=\"export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.5.0-jx2cc3ecnk4zkfz3kqwomwsm5rpfouiv/rlib/R && radian\"\nalias r351=\"export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.5.1-h4fvapuqf65avikja735xn5lzyildzzi/rlib/R && radian\"\nalias r352=\"export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.5.2-oxo76vo5ceamc2qp6nfr6zvcqstvzksb/rlib/R && radian\"\nalias r353=\"export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.5.3-a75bebsahiaaz6lrcdallnjprx3iybjv/rlib/R && radian\"\nalias r360=\"export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.6.0-2sbeu7xbnocpahdm7afuonsumblmgi5j/rlib/R && radian\"\nalias r361=\"export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.6.1-j25wr6zcofibs2zfjwg37357rjj26lqb/rlib/R && radian\""},{"path":"libraries.html","id":"rstudio-workbench","chapter":"2 Libraries and Environment Modules","heading":"2.5.2 RStudio Workbench","text":"RStudio Workbench, R interpreters installed within image.","code":""},{"path":"libraries.html","id":"rstudio-workbench-1","chapter":"2 Libraries and Environment Modules","heading":"2.6 RStudio Workbench","text":"RStudio Workbench running port 8787 (https://edi.geogr.uni-jena.de:8787) (legacy) directly https://edi.geogr.uni-jena.de/.service running docker container (Ubuntu 20.04 OS).\nHence, new terminal start terminal Ubuntu 20.04 underlying OS.\nservice decoupled CentOS host system.\nAlso R interpreters different ones used Slurm jobs environment modules via Spack.","code":""},{"path":"libraries.html","id":"shiny-server","chapter":"2 Libraries and Environment Modules","heading":"2.7 Shiny Server","text":"Shiny server running port 3838 user support.\nmeans users can place apps $HOME/Shinyapps/ deployed http://edi.geogr.uni-jena.de:3838/<username>/<appname>.Note http (https available Pro version).Exemplary apps:http://edi.geogr.uni-jena.de:3838/patrick/hyperspectral/http://edi.geogr.uni-jena.de:3838/jannes/cluster_map/","code":""},{"path":"libraries.html","id":"libraries-1","chapter":"2 Libraries and Environment Modules","heading":"2.8 Libraries","text":"find module want use, check Spack supports via spack list <module> contact admin install .\nfollowing subsections refer installations host, RStudio Workbench.","code":""},{"path":"libraries.html","id":"docker","chapter":"2 Libraries and Environment Modules","heading":"2.8.1 Docker","text":"Docker installed frontend (example convert .html .pdf).\nuse docker, user needs added “docker” group.\nPlease contact Admin case want use .","code":""},{"path":"libraries.html","id":"osrm","chapter":"2 Libraries and Environment Modules","heading":"2.8.2 OSRM","text":"Open Source Routing Machine installed.Start server osrm-routed.\ninformation can found .","code":""},{"path":"libraries.html","id":"gdal","chapter":"2 Libraries and Environment Modules","heading":"2.8.3 GDAL","text":"Check module avail look GDAL see versions available.","code":""},{"path":"libraries.html","id":"getting-started","chapter":"2 Libraries and Environment Modules","heading":"2.9 Getting Started","text":"worked steps, almost good go.\nrecommendations make life easier.","code":""},{"path":"libraries.html","id":"byobu","chapter":"2 Libraries and Environment Modules","heading":"2.9.0.1 byobu","text":"byobu wrapper SSH session makes possible close terminal session loose command running .\nstart long running jobs, can safely start byobu window without worrying quit shut machine.Run byobu logged byobu session launched.can open multiple ones byobu -S <session name>, e.g. byobu -S session2.\nmultiple ones open, interactive prompt ask one want start next time.","code":""},{"path":"libraries.html","id":"radian","chapter":"2 Libraries and Environment Modules","heading":"2.9.0.2 radian","text":"radian optimized R command line tool.\nnotice benefits compared default R start using .\nneed install via pip already installed installed python.Usually setuptools needs upgraded first.Now can either always use radian set alias .bashrc , e.g. alias r=\"radian\".\nNote radian works set env variable R_HOME correctly.\nSee details.work moment, might need add binary $PATH variable ~/.bashrc.","code":"pip install --user --upgrade setuptools\npip install --user radianexport PATH=~/.local/bin:$PATH"},{"path":"libraries.html","id":"ccache","chapter":"2 Libraries and Environment Modules","heading":"2.9.0.3 ccache","text":"load ccache, speed-source installations R packages lot.\n(Linux, R packages installed source.)\nBesides loading ccache, also need create following file home directory (~/.R/Makevars):(Note need create folder first, exist default (mkdir ~/.R/).)installing package now, occasionally see gcc lines prefixed ccache.\nmeans gcc call already executed now loaded cache rather run .\nsaves lot time, especially packages take long install (dplyr, Rcpp, stringi).","code":"CXX_STD = CXX14\n\nVER=\nCCACHE=ccache\nCC=$(CCACHE) gcc $(VER)\nCXX=$(CCACHE) g++$(VER)\nC11=$(CCACHE) g++$(VER)\nC14=$(CCACHE) g++$(VER)\nFC=$(CCACHE) gfortran$(VER)\nF77=$(CCACHE) gfortran$(VER)"},{"path":"libraries.html","id":"wrapper","chapter":"2 Libraries and Environment Modules","heading":"2.9.0.4 Create a bash alias for your project","text":"Often might want use {renv} library specific directory specific R version.\nRather navigating time hand loading R version manually, can create alias .\ncan course also use approach without {renv} - just load specific version.Put .bashrc save time :)","code":"alias my-project=\"cd /path/to/project && <load custom env module> && r361\""},{"path":"scheduler.html","id":"scheduler","chapter":"3 Scheduler","heading":"3 Scheduler","text":"SLURM responsible executing jobs across cluster.\ncontrast running “direct” R job command line server without scheduler, application takes care execution queue accounting running processes.R, done via R packages rzmq clustermq.\npackages provide interfaces zmq library fact work sending code requests language SLURM.\nrzmq package technical interface zmq combination, clustermq package used user start pipeline submitting job scheduler.Alternatives packages mentioned batchtools future.batchtools.","code":""},{"path":"scheduler.html","id":"first-steps","chapter":"3 Scheduler","heading":"3.1 First steps","text":"Rather calling R script directly, need wrap code function invoke using clustermq::Q().\nfirst might seem complex way run R code interactive line--line approach command line.\ntime value benefits approach:forced “functionize” code putting smaller pieces, ’ll diverge long R scriptsIn combination drake able track already built targets parallelize building intermediate targetsdrake successor targets provides interface clustermq giving possibility running multiple “targets” (can think one R script one intermediate step) parallel HPC.\n“targets” can parallelized.\ninformation see drake manual especially section HPC computing.highly recommended set SSH key (case done already) passwordless log .Even though {drake} highly recommend, want use , can also use clustermq::Q() directly.\n, way submit jobs compute nodes cluster using tools mentioned .See also section best practice information get started.","code":""},{"path":"scheduler.html","id":"slurm-commands","chapter":"3 Scheduler","heading":"3.2 SLURM commands","text":"execution jobs explained detail Chapter 4, following section aims familiarizing usage scheduler.basic SLURM commands aresinfo: overview current state nodessqueue: overview current jobs queued, including information running jobssacct: Overview jobs submitted past including end statescancel: Cancel running jobs using job ID identifier","code":"sinfo\n\nPARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST\nall*            up   infinite      4  alloc c[0-2],edi\nall*            up   infinite      2   idle c[3-4]\nfrontend        up   infinite      1  alloc edi\nthreadripper    up   infinite      4  alloc c[0-2],edi\nopteron         up   infinite      2   idle c[3-4]squeue\n\nJOBID     PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n129_[2-5]    threadripper  cmq7381  patrick PD       0:00      1 (Resources)\n121_2        threadripper  cmq7094  patrick  R    6:24:17      1 c1\n121_3        threadripper  cmq7094  patrick  R    6:24:17      1 c2\n129_1        threadripper  cmq7381  patrick  R    5:40:44      1 c0122             cmq7094     threadripper     (null)          0  COMPLETED      0:0\n123             cmq7094     threadripper     (null)          0    PENDING      0:0\n121             cmq7094     threadripper     (null)          0    PENDING      0:0\n125             cmq6623     threadripper     (null)          0     FAILED      1:0\n126             cmq6623     threadripper     (null)          0     FAILED      1:0\n127             cmq6623     threadripper     (null)          0     FAILED      1:0\n128             cmq6623     threadripper     (null)          0     FAILED      1:0\n124             cmq6623     threadripper     (null)          0     FAILED      1:0\n130             cmq7381     threadripper     (null)          0    PENDING      0:0"},{"path":"submit-jobs.html","id":"submit-jobs","chapter":"4 Submitting jobs","heading":"4 Submitting jobs","text":"","code":""},{"path":"submit-jobs.html","id":"clustermq-setup","chapter":"4 Submitting jobs","heading":"4.1 clustermq setup","text":"Every job submission done via clustermq::Q() (either directly via drake).\nSee setup instructions clustermq package setup package.First, need set options .Rprofile (master node project root use packrat):See package vignette set file.Note can multiple .Rprofile files system:default R interpreter use .Rprofile found home directory (~/).can also save .Rprofile file root directory (RStudio) project (preferred one $HOME).way can use customized .Rprofile files tailored project.stage able run example top README clustermq package.\nsimple example finishes seconds.\nwork, either something wrong nodes busy.\nCheck sinfo squeue.\nOtherwise see troubleshooting chapter.\naware setting n_cpus template argument clustermq::Q() submitted job parallelized! submit job parallelized without telling scheduler, scheduler reserve 1 core job (thinks sequential) fact multiple processes spawn. potentially affect running processes server since scheduler accept processing actually can take.\n","code":"\noptions(\n    clustermq.scheduler = \"slurm\",\n    clustermq.template = \"<\/path/to/file/\"\n)"},{"path":"submit-jobs.html","id":"the-scheduler-template","chapter":"4 Submitting jobs","heading":"4.2 The scheduler template","text":"successfully submit jobs scheduler, need set .Rprofile options given .\nNote can add bash commands scripts SBATCH section final R call.\nexample, one templates looks like :Note: # signs mistakes , “comment” signs context.\nSBATCH commands executed .can simply copy adjust needs (set right path project specify R version want use).","code":"#!/bin/sh\n#SBATCH --job-name={{ job_name }}\n#SBATCH --partition=all\n#SBATCH --output={{ log_file | /dev/null }} # you can add .%a for array index\n#SBATCH --error={{ log_file | /dev/null }}\n#SBATCH --cpus-per-task={{ n_cpus }}\n#SBATCH --mem={{ memory }}\n#SBATCH --array=1-{{ n_jobs }}\n\ncd path/to/project\n\n# load desired R version\nmodule load r-3.5.2-gcc-9.2.0-4syrmqv\n\nCMQ_AUTH={{ auth }} R --no-save --no-restore -e 'clustermq:::worker(\"{{ master }}\")'"},{"path":"submit-jobs.html","id":"allocating-resources","chapter":"4 Submitting jobs","heading":"4.3 Allocating resources","text":"two approaches/packages can use:drake (recommended)drake (recommended)clustermqclustermq(individual components calls explained detail .)Note drake uses clustermq hood.\nNotations like <X> meant read placeholders, meaning need replace valid content.)submitting jobs via clustermq::Q(), important tell scheduler many cores memory reserved .\nstep important.\nspecify less cores actually use script (e.g. internal parallelization), scheduler plan X cores although submitted code spawn Y processes background.\nmight overload node eventually cause script (importantly) processes others crash.two ways specify settings, depending approach use:via clustermq::Q() directlyPass values via argument template like template = list(n_cpus = <X>, memory = <Y>).\npassed clustermq.template file (frequently named slurm_clustermq.tmpl) contains following lines:tells scheduler many resources (cpus) job needs.via drake::make() (recommended), set options via argument template = list(n_cpus = X, memory = Y).\nSee section “resources column transient workers” drake manual.\nPlease think upfront many cpus memory task requires. following two examples show implications wrong specifications.\n\nmclapply(cores = 20) (script) > n_cpus = 16\n\ncase, four workers always “waiting mode” since 16 cpus can used resource request. slows parallelization harm users.\n\nmclapply(cores = 11) < n_cpus = 16\n\ncase, reserve 16 CPUs machine use 11 . blocks five CPUs machine reason potentially causing people added queue rather getting job processed immediately.\nFurthermore, want use resources node run memory problems, try reducing number CPUs (already increased memory maximum).\nscale number CPUs, memory/cpu available.","code":"drake::make(parallelism = \"clustermq\", n_jobs = 1, \n  template = list(n_cpus = <X>, log_file = <Y>, memory = <Z>))clustermq::Q(template = list(n_cpus = <X>, log_file = <Y>, memory = <Z>))#SBATCH --cpus-per-task{{ n_cpus }}\n#SBATCH --mem={{ memory }}"},{"path":"submit-jobs.html","id":"monitoring-progress","chapter":"4 Submitting jobs","heading":"4.4 Monitoring progress","text":"submitting jobs can track progress specifying log_file clustermq::Q() call, e.g. clustermq::Q(template = list(log_file = path//file)).drake, equivalent specify console_log_file() either make() drake_config().jobs running node, can SSH node, e.g. ssh c0.\ncan take look current load using htop.\nNote can log running progress specific node.Another option take look Ganglia see load HPC.","code":""},{"path":"submit-jobs.html","id":"summary","chapter":"4 Submitting jobs","heading":"4.5 Summary","text":"Set .Rprofile options(clustermq.template = \"/path//file\").\nclustermq.template point SLURM template file $HOME project directory.Set .Rprofile options(clustermq.template = \"/path//file\").\nclustermq.template point SLURM template file $HOME project directory.Decide approach want use:Decide approach want use:drake::make(parallelism = \"clustermq\", n_jobs = 1, template = list(n_cpus = X, log_file = Y, memory = Z)) (recommended!)clustermq::Q(template = list(n_cpus = X, log_file = Y, memory = Z))need Slurm template file project directory. template needs linked .Rprofile options(clustermq.template = \"/path//file\").","code":""},{"path":"submit-jobs.html","id":"best-practice","chapter":"4 Submitting jobs","heading":"4.6 Best practice","text":"can submit many tasks want separate R sessions, processed ranked queue scheduler.\nHowever, submitting single jobs finish quickly tedious.\napplies submission multiple jobs separate R sessions.two points reason highly recommended use drake.\nusing drake, intermediate R objects “targets”.\ncan specify number targets build calling drake::make().\ndrake take care target separate job created added SLURM queue.example, want build three R objects named object1, object2 object3 parallel, one node:Creates three jobs added SLURM queueEach jobs requires 16 coresEach job needs 32 GB memoryThe call finished jobs finished.done, marked “built” added cache directory drake, accidentally rebuild (unless change something substantial code).follow practice, thing need execute cluster call (initialized config sourcing drake.R file).","code":"\ndrake::make(plan, targets = c(\"object1\", \"object2\", \"object3\"), jobs = 3, \n            template = list(n_cpus = 16, memory = \"100G\", \n            log_file = \"/path/to/log.txt\"))"},{"path":"troubleshooting-and-cautionary-notes.html","id":"troubleshooting-and-cautionary-notes","chapter":"5 Troubleshooting and Cautionary Notes","heading":"5 Troubleshooting and Cautionary Notes","text":"","code":""},{"path":"troubleshooting-and-cautionary-notes.html","id":"cautionary-notes","chapter":"5 Troubleshooting and Cautionary Notes","heading":"5.1 Cautionary Notes","text":"","code":""},{"path":"troubleshooting-and-cautionary-notes.html","id":"faq","chapter":"5 Troubleshooting and Cautionary Notes","heading":"5.2 FAQ","text":"Question: ran sinfo saw state nodes “”.Answer: Please contact admin resume nodes.Question: ran sinfo saw nodes status “mix” “alloc”. differences?Answer: “mix” means node fully loaded mixed state processing idle. “alloc” means node fully allocated.Question: install udunits2. shall ?Answer: Install via install.packages('udunits2', type = 'source',  configure.args='---udunits2-lib=<path--installation>'.\nexample \"---udunits2-include=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/udunits2-2.2.24-hupsw3nzjxtem7wlwyugyga2uh4xgieo/include/\".can set option permanently ~/.Rprofile options(configure.args = list(udunits = c(<>))).Question: install rgdal | sf. shall ?Answer: spatial packages bit troublesome since expect certain libraries predefined directories. case, locations need given explicitly installation. need install custom options set configure.args, similar udunits .following works:Shotts, William E. 2012. Linux Command Line: Complete Introduction. San Francisco: Starch Press.Sobell, Mark G. 2010. Practical Guide Linux Commands, Editors, Shell Programming. 2nd ed. Upper Saddle River, NJ: Prentice Hall.Ward, Brian. 2015. Linux Works: Every Superuser Know. 2nd edition. San Francisco: Starch Press.","code":"\noptions(\n  configure.args=list(\n    rgdal = c(\n      \"--with-proj-lib=/opt/spack/opt/spack/linux-centos7-zen/gcc-9.2.0/proj-5.2.0-az6mkj55zpnh6fmg2ae5wyrmhfiynxfx/lib\"),\n    sf = c(\n      \"--with-proj-lib=/opt/spack/opt/spack/linux-centos7-zen/gcc-9.2.0/proj-5.2.0-az6mkj55zpnh6fmg2ae5wyrmhfiynxfx/lib\")\n  )\n)"}]
