[{"path":"index.html","id":"intro","chapter":"1 Introduction","heading":"1 Introduction","text":"Welcome user manual High-Performance-Cluster (HPC) GIScience group (University Jena).tailored towards R processing can also used programming language.short introduction HPCsThe big advantage HPC users can submit jobs ONE machine distributes work across multiple machines background.\nIncoming processing requests (jobs) handled scheduler (SLURM), taking away work queuing job potential issue clashing jobs users.Administration simplified provisioning computing nodes virtual image.\nway, maintenance tasks reduced differences machines avoided.\nLibrary management across nodes done via Spack package manager application allows version-agnostic environment module installations.$HOME directory every user shared across nodes, avoiding need keep data scripts sync across multiple machines.start:Working Linux server naturally requires certain amount familiarity UNIX command-line shells text editors.\ndozens Linux online tutorials help get started.1\ncourse, also great books use Linux Shotts (2012), Sobell (2010) Ward (2015) freely available.\nstill get stuck, Google certainly help .Working Linux server naturally requires certain amount familiarity UNIX command-line shells text editors.\ndozens Linux online tutorials help get started.1\ncourse, also great books use Linux Shotts (2012), Sobell (2010) Ward (2015) freely available.\nstill get stuck, Google certainly help .Please add SSH key pair account able log server without type password every time.\nespecially useful since password consist many letters numbers (> 10) want memorize.\nSee guide never worked SSH keys .\nalready use SSH key pair machine, can use ssh-copy-id <username>@10.232.16.28 copy key server.\nAfterwards able login via ssh <username>@10.232.16.28 without prompted password.Please add SSH key pair account able log server without type password every time.\nespecially useful since password consist many letters numbers (> 10) want memorize.\nSee guide never worked SSH keys .\nalready use SSH key pair machine, can use ssh-copy-id <username>@10.232.16.28 copy key server.\nAfterwards able login via ssh <username>@10.232.16.28 without prompted password.","code":""},{"path":"index.html","id":"web-address","chapter":"1 Introduction","heading":"1.1 Web Address","text":"https://edi.geogr.uni-jena.deIP: 10.232.16.28","code":""},{"path":"index.html","id":"hardware","chapter":"1 Introduction","heading":"1.2 Hardware","text":"cluster consists following machines:Group “threadripper”:CPU: AMD Threadripper 2950X, 16-core, Hyperthreading support, 3.5 GHz - 4.4 GHzRAM: 126 GB DDR4Number nodes: 4 c[0-2] (+ frontend)“frontend” operating 12 cores 100 GB RAMGroup “opteron”:CPU: AMD Opteron 6172, 48 cores, Hyperthreading, 2.1 GHzRAM: 252 GB DDR3 (c5 comes 130 GB RAM)Number nodes:\n2 (c[3-4])\n1 (c5)\n2 (c[3-4])1 (c5)groups reflected scheduler via “partition” setting.Group “threadripper” 3.5x faster group “opteron”.","code":""},{"path":"index.html","id":"software","chapter":"1 Introduction","heading":"1.3 Software","text":"HPC built following installation guide provided Open HPC community (using “Warewulf + Slurm” edition) operates CentOS 7 base.\nscheduler used queuing processing job requests SLURM.\nLoad monitoring performed via Ganglia.\nlive view accessible .\nSpack used package manager.\ndetailed instructions scheduler package manager can found respective chapters.","code":""},{"path":"index.html","id":"data-storage","chapter":"1 Introduction","heading":"1.4 Data storage","text":"mars data server mounted /home stores data.\nCurrently capacity 20 TB users combined.\nData can stored directly /home directory.","code":""},{"path":"index.html","id":"accessing-files-from-your-local-computer","chapter":"1 Introduction","heading":"1.5 Accessing files from your local computer","text":"recommended mount server via sshfs local machine.\nTransfer speed ranges 50 - 100 Mbit/s ’re office able access files without delay.\nAccessing files outside slower.really run trouble transfer speed, directly connect mars server.Otherwise, route follows: <local> (sshfs) -> edi (nfs) -> mars","code":""},{"path":"index.html","id":"unix","chapter":"1 Introduction","heading":"1.5.1 Unix","text":"Unix system, following command can usedThe mount process passwordless via SSH (.e. via ~/.ssh/id_rsa key).\nNote mount actually performed root user, need copy SSH key root user: cp ~/.ssh/id_rsa /root/.ssh/id_rsa.convenience can create executable script performs action every time need .\nAuto-mount boot via fstab recommended since sometimes network yet mount executed. applies especially office accessing server outside.\n","code":"sudo sshfs -o reconnect,idmap=user,transform_symlinks,identityFile=~/.ssh/id_rsa,allow_other,cache=yes,kernel_cache,compression=no,default_permissions,uid=1000,gid=100,umask=0 <username>@10.232.16.28:/ <local-directory>"},{"path":"index.html","id":"windows","chapter":"1 Introduction","heading":"1.5.2 Windows","text":"Please install sshfs-win follow instructions.","code":""},{"path":"index.html","id":"accessing-the-hpc-remotely-vpn","chapter":"1 Introduction","heading":"1.6 Accessing the HPC remotely (VPN)","text":"VPN connection required access HPC remotely.\nFSU uses Cisco Anyconnect protocol VPN purposes recommends use “Cisco Anyconnect Client”.GUI requires manual connect/disconnect actions.\naddition, Cisco Anyconnect client known slow laggy (sorry references , just personal experience).\nopen-source alternative openconnect much better job - comes powerful CLI implementation.\nGUI preferred, look (looks like Windows way around GUI?).InstallationmacOS: brew install openconnectWindows: choco install openconnect-gui downloading GUI directlyUsageOn UNIX-based system:Start: echo <PASSWORD> | sudo openconnect --user=<USER>@uni-jena.de --passwd--stdin --background vpn.uni-jena.de\nStop: sudo killall -SIGINT openconnectOn Windows: Sorry, using Windows - feel free add info .","code":""},{"path":"rstudio-workbench.html","id":"rstudio-workbench","chapter":"2 RStudio Workbench","heading":"2 RStudio Workbench","text":"RStudio Workbench (RSW) running main node cluster.\naims central go-place scripts analysis development.RSW running Docker container running Ubuntu 20.04.\nmeans open terminal RSW, Ubuntu environment.\ndifferent environment ssh directly onto server, running CentOS7 system.\npackages installed RSW linked Ubuntu 20.04 system libraries already installed RSW container.RSW running main node 32 cores 124 GB RAM.\nmachine shared multiple people, please ensure use much memory clean time time.\nRSW shows memory usage “Environment” pane IDE.\nBesides, can go terminal execute htop check current load server.","code":""},{"path":"rstudio-workbench.html","id":"slurm","chapter":"2 RStudio Workbench","heading":"2.1 SLURM","text":"RSW connected SLURM scheduler able add jobs SLURM queue.","code":""},{"path":"rstudio-workbench.html","id":"python","chapter":"2 RStudio Workbench","heading":"2.2 Python","text":"RSW ships support JupyterLab JupyterNotebooks.\nlaunch Jupyter session selecting drop-new session instead “RStudio”.","code":""},{"path":"rstudio-workbench.html","id":"vscode","chapter":"2 RStudio Workbench","heading":"2.3 VSCode","text":"RSW >= 1.4 ships full VSCode integration.\nlaunch VSCode session selecting drop-new session instead “RStudio”.","code":""},{"path":"shiny-server.html","id":"shiny-server","chapter":"3 Shiny Server","heading":"3 Shiny Server","text":"Shiny server running port 3838 user support.\nmeans users can place apps $HOME/Shinyapps/ deployed http://edi.geogr.uni-jena.de:3838/<username>/<appname>.Note http (https available Pro version).Exemplary apps:http://edi.geogr.uni-jena.de:3838/patrick/hyperspectral/http://edi.geogr.uni-jena.de:3838/jannes/cluster_map/","code":""},{"path":"slurm-hpc-scheduler.html","id":"slurm-hpc-scheduler","chapter":"4 SLURM: HPC scheduler","heading":"4 SLURM: HPC scheduler","text":"written scripts want execute , advisable send scheduler.\nscheduler (SLURM) distribute jobs across cluster (6+ machines) make sure conflicts respect CPU memory multiple people send jobs cluster.\nessential job scheduler.","code":""},{"path":"slurm-hpc-scheduler.html","id":"first-steps","chapter":"4 SLURM: HPC scheduler","heading":"4.1 First steps","text":"Sending jobs SLURM R supported via R package clustermq.\nR interpreters packages shared RSW. Therefore, R packages script needs must reinstalled HPC respective R version.\nRather calling R script directly, need wrap code function invoke using clustermq::Q().\nInstead using clustermq directly, can make use R packages like {targets} {drake} automatically wrap whole analysis way executes layers analysis HPC.way submit R jobs compute nodes cluster using tools mentioned .Also, essential load required system libraries need (e.g. GDAL, PROJ) via environment modules available nodes.\nNote likely versions libraries differ ones used RSW container. reproducibility might worth deviating much even using versions HPC within RSW.\n","code":""},{"path":"slurm-hpc-scheduler.html","id":"slurm-commands","chapter":"4 SLURM: HPC scheduler","heading":"4.2 SLURM commands","text":"execution jobs explained detail Chapter 4, following section aims familiarizing usage scheduler.\nscheduler queried via terminal, .e. need ssh server switch “Terminal” tab RStudio.important SLURM commands aresinfo: overview current state nodessqueue: overview current jobs queued, including information running jobssacct: Overview jobs submitted past including end statescancel: Cancel running jobs using job ID identifier\nwant cancel jobs specific user, can call scancel -u <username>.scancel: Cancel running jobs using job ID identifierIf want cancel jobs specific user, can call scancel -u <username>.","code":"sinfo\n\nPARTITION    AVAIL  TIMELIMIT  NODES  STATE NODELIST\nall*            up   infinite      4  alloc c[0-2],edi\nall*            up   infinite      2   idle c[3-4]\nfrontend        up   infinite      1  alloc edi\nthreadripper    up   infinite      4  alloc c[0-2],edi\nopteron         up   infinite      2   idle c[3-4]squeue\n\nJOBID     PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n129_[2-5]    threadripper  cmq7381  patrick PD       0:00      1 (Resources)\n121_2        threadripper  cmq7094  patrick  R    6:24:17      1 c1\n121_3        threadripper  cmq7094  patrick  R    6:24:17      1 c2\n129_1        threadripper  cmq7381  patrick  R    5:40:44      1 c0122             cmq7094     threadripper     (null)          0  COMPLETED      0:0\n123             cmq7094     threadripper     (null)          0    PENDING      0:0\n121             cmq7094     threadripper     (null)          0    PENDING      0:0\n125             cmq6623     threadripper     (null)          0     FAILED      1:0\n126             cmq6623     threadripper     (null)          0     FAILED      1:0\n127             cmq6623     threadripper     (null)          0     FAILED      1:0\n128             cmq6623     threadripper     (null)          0     FAILED      1:0\n124             cmq6623     threadripper     (null)          0     FAILED      1:0\n130             cmq7381     threadripper     (null)          0    PENDING      0:0"},{"path":"slurm-hpc-scheduler.html","id":"submit-jobs","chapter":"4 SLURM: HPC scheduler","heading":"4.3 Submitting jobs","text":"","code":""},{"path":"slurm-hpc-scheduler.html","id":"clustermq-setup","chapter":"4 SLURM: HPC scheduler","heading":"4.3.1 clustermq setup","text":"Every job submission done via clustermq::Q() (either directly via drake).\nSee setup instructions clustermq package setup package.First, need set options .Rprofile (master node project root use {renv} {packrat}):See package vignette set file.Note can multiple .Rprofile files system:default R interpreter use .Rprofile found home directory (~/).can also save .Rprofile file root directory (RStudio) project (preferred one $HOME).way can use customized .Rprofile files tailored project.stage able run example top README {clustermq} package.\nsimple example finishes seconds.\nwork, either something wrong nodes busy.\nCheck sinfo squeue.\nOtherwise see troubleshooting chapter.\naware setting n_cpus template argument clustermq::Q() submitted job parallelized! submit job parallelized without telling scheduler, scheduler reserve 1 core job (thinks sequential) fact multiple processes spawn. potentially affect running processes server since scheduler accept processing actually can take.\n","code":"\noptions(\n    clustermq.scheduler = \"slurm\",\n    clustermq.template = \"<\/path/to/file/\"\n)"},{"path":"slurm-hpc-scheduler.html","id":"the-scheduler-template","chapter":"4 SLURM: HPC scheduler","heading":"4.3.2 The scheduler template","text":"successfully submit jobs scheduler, need set .Rprofile options given .\nNote can add bash commands scripts SBATCH section final R call.example, template look follows:Note: # signs mistakes , “comment” signs context.\nSBATCH commands executed .can simply copy adjust needs.\nneed set right path project specify R version want use.","code":"#!/bin/sh\n#SBATCH --job-name={{ job_name }}\n#SBATCH --partition=all\n#SBATCH --output={{ log_file | /dev/null }} # you can add .%a for array index\n#SBATCH --error={{ log_file | /dev/null }}\n#SBATCH --cpus-per-task={{ n_cpus }}\n#SBATCH --mem={{ memory }}\n#SBATCH --array=1-{{ n_jobs }}\n\ncd /full/path/to/project\n\n# load desired R version via an env module\nmodule load r-3.5.2-gcc-9.2.0-4syrmqv\n\nCMQ_AUTH={{ auth }} R --no-save --no-restore -e 'clustermq:::worker(\"{{ master }}\")'"},{"path":"slurm-hpc-scheduler.html","id":"allocating-resources","chapter":"4 SLURM: HPC scheduler","heading":"4.3.3 Allocating resources","text":"two approaches/packages can use:drake / targetsdrake / targetsclustermqclustermqThe drake approach valid set project drake targets project.(individual components calls explained detail .)Note drake uses clustermq hood.\nNotations like <X> meant read placeholders, meaning need replaced valid content.)submitting jobs via clustermq::Q(), important tell scheduler many cores memory reserved .\nstep important.specify less cores actually use script (e.g. internal parallelization), scheduler plan X cores although submitted code spawn Y processes background.\nmight overload node eventually cause script (importantly) processes others crash.two ways specify settings, depending approach use:via clustermq::Q() directlyPass values via argument template like template = list(n_cpus = <X>, memory = <Y>).\npassed clustermq.template file (frequently named slurm_clustermq.tmpl) contains following lines:tells scheduler many resources (cpus) job needs.via drake::make(), set options via argument template = list(n_cpus = X, memory = Y).\nSee section “resources column transient workers” drake manual.\nPlease think upfront many cpus memory task requires. following two examples show implications wrong specifications.\n\nmclapply(cores = 20) (script) > n_cpus = 16\n\ncase, four workers always “waiting mode” since 16 cpus can used resource request. slows parallelization harm users.\n\nmclapply(cores = 11) < n_cpus = 16\n\ncase, reserve 16 CPUs machine use 11 . blocks five CPUs machine reason potentially causing people added queue rather getting job processed immediately.\nFurthermore, want use resources node run memory problems, try reducing number CPUs (already increased memory maximum).\nscale number CPUs, memory/cpu available.","code":"drake::make(parallelism = \"clustermq\", n_jobs = 1, \n  template = list(n_cpus = <X>, log_file = <Y>, memory = <Z>))clustermq::Q(template = list(n_cpus = <X>, log_file = <Y>, memory = <Z>))#SBATCH --cpus-per-task{{ n_cpus }}\n#SBATCH --mem={{ memory }}"},{"path":"slurm-hpc-scheduler.html","id":"monitoring-progress","chapter":"4 SLURM: HPC scheduler","heading":"4.3.4 Monitoring progress","text":"submitting jobs can track progress specifying log_file clustermq::Q() call, e.g. clustermq::Q(template = list(log_file = path//file)).drake, equivalent specify console_log_file() either make() drake_config().jobs running node, can SSH node, e.g. ssh c0.\ncan take look current load using htop.\nNote can log running progress specific node.","code":""},{"path":"slurm-hpc-scheduler.html","id":"summary","chapter":"4 SLURM: HPC scheduler","heading":"4.4 Summary","text":"Set .Rprofile options(clustermq.template = \"/path//file\").\nclustermq.template point SLURM template file $HOME project directory.Set .Rprofile options(clustermq.template = \"/path//file\").\nclustermq.template point SLURM template file $HOME project directory.Decide approach want use drake/targets clustermqDecide approach want use drake/targets clustermqA Slurm template file required.\ntemplate needs linked .Rprofile options(clustermq.template = \"/path//file\").Slurm template file required.\ntemplate needs linked .Rprofile options(clustermq.template = \"/path//file\").","code":""},{"path":"libraries.html","id":"libraries","chapter":"5 Libraries and Environment Modules","heading":"5 Libraries and Environment Modules","text":"","code":""},{"path":"libraries.html","id":"introduction","chapter":"5 Libraries and Environment Modules","heading":"5.1 Introduction","text":"Environment modules needed make certain libraries (e.g. R) available across nodes cluster.Environment variables installed maintained via Spack.\nPlease note RStudio Workbench decoupled Slurm. Everything run nodes via Slurm requires respective environment modules loaded Slurm template file whereas run something RStudio Workbench, libraries installed underlying Ubuntu 20.04 container used.\nfollowing part deals Spack configuration side everything container just work (respect R).Available env modules can queried terminal via modules avail.able load modules, put following top ~/.bashrc file:(export MODULEPATH env var actually done spack setup script failed past users. Adding manually harm.)sample output module avail:","code":"export SPACK_ROOT=/opt/spack\n. $SPACK_ROOT/share/spack/setup-env.sh\n\nexport MODULEPATH=/opt/spack/share/spack/modules/linux-centos7-x86_64------------------------- /opt/spack/share/spack/modules/linux-centos7-x86_64 --------------------------\n   byobu-5.127-gcc-9.2.0-by2qc2g          (L)    python-3.7.4-gcc-9.2.0-nbjbfzi     (L)\n   ccache-3.3.4-gcc-9.2.0-v3xzqqh         (L)    r-3.5.2-gcc-9.2.0-oxo76vo\n   curl-7.63.0-gcc-9.2.0-cq4w37y          (L)    r-3.6.1-gcc-9.2.0-j25wr6z\n   fish-3.0.0-gcc-9.2.0-gdyab6r      "},{"path":"libraries.html","id":"loading-modules","chapter":"5 Libraries and Environment Modules","heading":"5.2 Loading modules","text":"Modules can loaded via module load <module>.First, C compiler need loaded libraries depend .Next, load libraries analysis/R packages need, example GDAL, PROJ, etc.\nload modules within ~/.bashrc within SLURM template.\n","code":"module load gcc-9.2.0-gcc-4.8.5-wqdecm4"},{"path":"miscellaneous-helpers.html","id":"miscellaneous-helpers","chapter":"6 Miscellaneous Helpers","heading":"6 Miscellaneous Helpers","text":"worked steps, almost good go.\nadditional recommendations make server life easier.","code":""},{"path":"miscellaneous-helpers.html","id":"byobu","chapter":"6 Miscellaneous Helpers","heading":"6.1 byobu","text":"byobu wrapper SSH session makes possible close terminal session loose command running .\nstart long running jobs, can safely start byobu window without worrying quit shut machine.Run byobu logged byobu session launched.can open multiple ones byobu -S <session name>, e.g. byobu -S session2.\nmultiple ones open, interactive prompt ask one want start next time.","code":""},{"path":"miscellaneous-helpers.html","id":"radian","chapter":"6 Miscellaneous Helpers","heading":"6.2 radian","text":"radian optimized R command line tool.\nnotice benefits compared default R start using .\nneed install via pip already installed installed python.Usually setuptools needs upgraded first.Now can either always use radian set alias .bashrc , e.g. alias r=\"radian\".\nNote radian works set env variable R_HOME correctly.\nSee details.work moment, might need add binary $PATH variable ~/.bashrc.","code":"pip3 install --user --upgrade setuptools\npip3 install --user radianexport PATH=~/.local/bin:$PATH"},{"path":"miscellaneous-helpers.html","id":"ccache","chapter":"6 Miscellaneous Helpers","heading":"6.3 ccache","text":"load ccache, speed-source installations R packages lot.\n(Linux, R packages installed source.)\nBesides loading ccache, also need create following file home directory (~/.R/Makevars):(Note need create folder first, exist default (mkdir ~/.R/).)installing package now, occasionally see gcc lines prefixed ccache.\nmeans gcc call already executed now loaded cache rather run .\nsaves lot time, especially packages take long install (dplyr, Rcpp, stringi).","code":"CXX_STD = CXX14\n\nVER=\nCCACHE=ccache\nCC=$(CCACHE) gcc $(VER)\nCXX=$(CCACHE) g++$(VER)\nC11=$(CCACHE) g++$(VER)\nC14=$(CCACHE) g++$(VER)\nFC=$(CCACHE) gfortran$(VER)\nF77=$(CCACHE) gfortran$(VER)"},{"path":"miscellaneous-helpers.html","id":"using-rstudio-package-manager-binaries","chapter":"6 Miscellaneous Helpers","heading":"6.4 Using RStudio Package Manager binaries","text":"Instead installing source, can speed R package installations lot making use RSPM binaries.\nalready configured default RSW.\nmake use HPC, put following ~/.Rprofile:","code":"\n# otherwise RSPM does not detect CentOS7 OS\noptions(HTTPUserAgent = sprintf(\"R/%s R (%s)\", getRversion(), paste(getRversion(), R.version$platform, R.version$arch, R.version$os)))\n# set RSPM repo\noptions(repos = structure(c(CRAN = \"https://packagemanager.rstudio.com/cran/__linux__/centos7/latest\")))"},{"path":"miscellaneous-helpers.html","id":"wrapper","chapter":"6 Miscellaneous Helpers","heading":"6.5 Create a bash alias for your project","text":"Often might want use {renv} library specific directory specific R version logged server.Rather navigating time hand loading R version manually, can create alias .\ncan course also use approach without {renv} - just load specific version.","code":"alias my-project=\"cd /path/to/project && <load custom R env module> && R\""},{"path":"troubleshooting-and-cautionary-notes.html","id":"troubleshooting-and-cautionary-notes","chapter":"7 Troubleshooting and Cautionary Notes","heading":"7 Troubleshooting and Cautionary Notes","text":"","code":""},{"path":"troubleshooting-and-cautionary-notes.html","id":"cautionary-notes","chapter":"7 Troubleshooting and Cautionary Notes","heading":"7.1 Cautionary Notes","text":"","code":""},{"path":"troubleshooting-and-cautionary-notes.html","id":"faq","chapter":"7 Troubleshooting and Cautionary Notes","heading":"7.2 FAQ","text":"Question: ran sinfo saw state nodes “”.Answer: Please contact admin resume nodes.Question: ran sinfo saw nodes status “mix” “alloc”. differences?Answer: “mix” means node fully loaded mixed state processing idle. “alloc” means node fully allocated.","code":""},{"path":"admin-tasks.html","id":"admin-tasks","chapter":"8 Admin Tasks","heading":"8 Admin Tasks","text":"","code":""},{"path":"admin-tasks.html","id":"warewulf","chapter":"8 Admin Tasks","heading":"8.1 Warewulf","text":"Cluster management.","code":""},{"path":"admin-tasks.html","id":"enable-systemd-services-on-nodes","chapter":"8 Admin Tasks","heading":"8.1.1 Enable systemd services on nodes","text":"","code":"pdsh -w c[0-5] systemctl <command>"},{"path":"admin-tasks.html","id":"enable-systemd-service-in-image","chapter":"8 Admin Tasks","heading":"8.1.2 Enable systemd service in image","text":"","code":"export CHROOT=<some path>\nchroot $CHROOT systemctl enable <service>"},{"path":"admin-tasks.html","id":"updating-image-nodes","chapter":"8 Admin Tasks","heading":"8.1.3 Updating image nodes","text":"/root/update-nodes.sh","code":"#!/bin/bash\n\nif [ \"$EUID\" -ne 0 ]\n  then echo \"Please run as root\"\n  exit\nfi\nIMAGE_NAME=${1?Error: no Image name given. Please pass something like <centos7.7>.}\n\n#backup\n\nexport CHROOT=/opt/ohpc/admin/images/$IMAGE_NAME\nyum -y upgrade\nyum -y --installroot=$CHROOT upgrade\nwwvnfs --chroot $CHROOT\nwwsh provision set c[0-5] --vnfs=$IMAGE_NAME\nKERNEL=$(eval \"uname -r\")\nwwbootstrap $KERNEL\nwwsh provision set c[0-5] --bootstrap=$KERNEL\n\npdsh -w c[0-5] reboot\nsleep 90\n\n# somehow munge does not create the log dir itslef\npdsh -w c[0-5] mkdir /var/log/munge\npdsh -w c[0-5] chown -R munge:munge /var/log/munge\npdsh -w c[0-5] systemctl restart munge\nscontrol update NodeName=c[0-5] State=DOWN Reason=\"undraining\"\nscontrol update NodeName=c[0-5] state=resume"},{"path":"admin-tasks.html","id":"slurm-1","chapter":"8 Admin Tasks","heading":"8.2 SLURM","text":"","code":""},{"path":"admin-tasks.html","id":"undrain-a-node","chapter":"8 Admin Tasks","heading":"8.2.1 Undrain a node","text":"node state “drain”, one can undrain via","code":"scontrol update NodeName=<node> State=DOWN Reason=\"undraining\"\nscontrol update NodeName=<node> State=RESUME"},{"path":"admin-tasks.html","id":"reconfigure-slurm","chapter":"8 Admin Tasks","heading":"8.2.2 Reconfigure Slurm","text":"E.g. settings update","code":"scontrol reconfigure"},{"path":"admin-tasks.html","id":"docker","chapter":"8 Admin Tasks","heading":"8.3 Docker","text":"","code":""},{"path":"admin-tasks.html","id":"pulling-a-new-image","chapter":"8 Admin Tasks","heading":"8.3.1 Pulling a new image","text":"Via user admingeogr AWS pull credentials configured","code":"cd /home/admingeogr/rsw\ndocker-compose pull"},{"path":"admin-tasks.html","id":"update-a-container","chapter":"8 Admin Tasks","heading":"8.3.2 Update a container","text":"","code":"cd /home/admingeogr/rsw\ndocker-compose up -d"},{"path":"admin-tasks.html","id":"clean-up-old-images","chapter":"8 Admin Tasks","heading":"8.3.3 Clean up old images","text":"Shotts, William E. 2012. Linux Command Line: Complete Introduction. San Francisco: Starch Press.Sobell, Mark G. 2010. Practical Guide Linux Commands, Editors, Shell Programming. 2nd ed. Upper Saddle River, NJ: Prentice Hall.Ward, Brian. 2015. Linux Works: Every Superuser Know. 2nd edition. San Francisco: Starch Press.","code":"docker image prune -af"}]
