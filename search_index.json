[
["index.html", "HPC User Guide Chapter 1 Introduction 1.1 Web Address 1.2 Hardware 1.3 Software 1.4 Data storage 1.5 Accessing files from your local computer 1.6 Sharing of Results", " HPC User Guide Patrick Schratz, Jannes Muenchow 2019-10-02 Chapter 1 Introduction Welcome to the user manual of the High-Performance-Server (HPC) of the GIScience group (University Jena). It is tailored towards R processing. This document describes how to get started and submit jobs to the cluster. A short introduction to HPCs The big advantage of a HPC is that users can submit jobs to ONE machine which then distributes the work across multiple machines in the background. Incoming processing requests (jobs) are handled by the scheduler (SLURM), taking away the work of queuing the job and the potential issue of clashing into jobs from other users. Administration is simplified by provisioning all computing nodes with the same virtual image. This way, maintenance tasks are reduced and differences between the machines are avoided. Administration is further simplified by using the Spack package manager as this application allows for version-agnostic environment module installations. The $HOME directory of every user is shared across all nodes, avoiding the need to keep data and scripts in sync across multiple machines. Before you start: Working on a Linux server naturally requires a certain amount of familiarity with UNIX command-line shells and text editors. There are dozens of Linux online tutorials which should help to get you started.1 Of course, there are also great books on how to use Linux such as Shotts (2012), Sobell (2010) and Ward (2015) all of which are freely available. If you still get stuck, Google might help you. Please add a SSH key pair to your account to be able to log in to the server without having to type your password. This is especially useful since your password will consist of many letters and numbers (&gt; 10) which you do not want to memorize. See this guide if you have never worked with SSH keys before. If you already use a SSH key pair on your machine, you can use ssh-copy-id &lt;username&gt;@141.35.158.107 to copy your key to the server. Afterwards you should be able to login via ssh &lt;username&gt;@141.35.158.107 without being prompted for your password. 1.1 Web Address https://edi.geogr.uni-jena.de IP: 141.35.158.107 1.2 Hardware The cluster consists of the following machines: Group “threadripper”: CPU: AMD Threadripper 2950X, 16-core, Hyperthreading support, 3.5 GHz - 4.4 GHz RAM: 126 GB DDR4 Number of nodes: 4 c[0-2] (+ frontend) The “frontend” is only operating on 12 cores with 100 GB RAM Group “opteron”: CPU: AMD Opteron 6172, 48 cores, no Hyperpthreading, 2.1 GHz RAM: 252 GB DDR3 Number of nodes: 2 (c[3-4]) The groups are reflected in the scheduler via the “partition” setting. Group “threadripper” is about 3.5x faster than group “opteron”. 1.3 Software The HPC was built following the installation guide provided by the Open HPC community (using the “Warewulf + Slurm” edition) and operates on a CentOS 7 base. The scheduler that is used for queuing processing job requests is SLURM. Load monitoring is performed via Ganglia. A live view is accessible here. Spack is used as the package manager. More detailed instructions on the scheduler and the package manager can be found in their respective chapters. 1.4 Data storage The mars data server is mounted at /home and stores all the data. Currently we have a capacity of 20 TB for all users combined. Data can be stored directly under your /home directory. 1.5 Accessing files from your local computer It is recommended to mount the server via sshfs to your local machine. Transfer speed ranges between 50 - 100 Mbit/s when you’re in the office so you should be able to access files without a delay. Accessing files from outside will be slower. If you really run in trouble with transfer speed, you could directly connect to the mars server. Otherwise, the route is as follows: &lt;local&gt; (sshfs) -&gt; edi (nfs) -&gt; mars 1.5.1 Unix For Unix system, the following command can be used sudo sshfs -o reconnect,idmap=user,transform_symlinks,identityFile=~/.ssh/id_rsa,allow_other,cache=yes,kernel_cache,compression=no,default_permissions,uid=1000,gid=100,umask=0 &lt;username&gt;@141.35.158.107:/ &lt;local-directory&gt; The mount process is passwordless if you do it via SSH (i.e. via your ~/.ssh/id_rsa key). Note that the mount is actually performed by the root user, so you need to copy your SSH key to the root user: cp ~/.ssh/id_rsa /root/.ssh/id_rsa. For convenience you can create an executable script that performs this action every time you need it. Auto-mount during boot via fstab is not recommended since sometimes the network is not yet up when the mount is executed. This applies especially if you are not in the office but accessing the server from outside. 1.5.2 Windows Please install sshfs-win and follow the instructions. 1.6 Sharing of Results 1.6.1 workflowr Take a look at the R package workflowr which allows automatic deployment of R Notebooks. 1.6.2 Local web server The Jupiter server (141.35.159.87) is set up as a web-server. This means it is able to render HTML contents and list files in a directory listing. Jupiter is mounted to the cluster at /mnt/nfs/jupiter. To use the functionality, you simply need to copy your desired files to /home/www/&lt;folder&gt; on Jupiter. To do so, please follow these steps: Request an account on Jupiter from Andreas Login to Jupiter and create the desired folders. The public URL relates as follows to the local directory: https://jupiter.geogr.uni-jena.de/&lt;folder&gt; -&gt; /home/www/&lt;folder&gt; Copy your files from the cluster to the desired location on Jupiter using the following code rsync -rlptDvzog --chown=www-data:www-data --fake-super \\ /mnt/nfs/jupiter/&lt;path-to-your-file&gt; \\ -e ssh &lt;username&gt;@jupiter.geogr.uni-jena.de:/home/www/&lt;path-to-directory&gt; This code will only copy files that have changed when being compared to the last execution so you can safely automate the call into your workflow. Since all these contents will be available to everyone, you want to add a README.md file to your directory listing as well as an Impressum. Even though no one usually knows this URL by default, be careful about sharing sensible files. See https://jupiter.geogr.uni-jena.de/life-healthy-forest/ for an example. You can request this theme from Andreas. References "],
["libraries.html", "Chapter 2 Libraries and Environment Modules 2.1 Introduction 2.2 Loading modules 2.3 Checking loaded modules 2.4 Default modules 2.5 R 2.6 RStudio Server 2.7 Shiny Server 2.8 Libraries 2.9 Getting Started", " Chapter 2 Libraries and Environment Modules 2.1 Introduction Most system relevant libraries are installed via Spack. However, as a user you do not need to worry about this since you are loading everything via “environment modules”. Available modules can be queried using modules avail. For setup reasons, please put the following at the top of your ~/.bashrc file export SPACK_ROOT=/opt/spack . $SPACK_ROOT/share/spack/setup-env.sh Please use only module list with the suffix “linux-centos7-zen”. Here is a snapshot: ------------------------- /opt/spack/share/spack/modules/linux-centos7-zen ------------------------- byobu-5.127-gcc-9.2.0-x5e5mnz (L) pkg-config-0.29.2-gcc-9.2.0-tsjdiyy ccache-3.3.4-gcc-9.2.0-7klqklq (L) proj-4.9.2-gcc-9.2.0-3skzcql cmake-3.15.3-gcc-9.2.0-e7uy3lq proj-5.2.0-gcc-9.2.0-az6mkj5 (L) curl-7.63.0-gcc-9.2.0-zqkxl74 (L) proj-6.1.0-gcc-9.2.0-i25e6py gdal-2.4.2-gcc-9.2.0-vbgkeil r-rgdal-1.4-4-gcc-9.2.0-elwh3bl gdal-3.0.1-gcc-9.2.0-2rbxot4 (L) r-rgdal-1.4-4-gcc-9.2.0-u3ixmgg geos-3.7.2-gcc-9.2.0-hn6gflr (L) sqlite-3.29.0-gcc-9.2.0-abag2du neovim-0.3.4-gcc-9.2.0-tsvynsq (L) zlib-1.2.11-gcc-9.2.0-yuybfrv (L) openblas-0.3.7-gcc-9.2.0-gwikytn (L) ------------------------------------ /opt/ohpc/pub/modulefiles ------------------------------------- EasyBuild/3.9.2 clustershell/1.8.1 hwloc/2.0.3 pmix/2.2.2 valgrind/3.15.0 autotools cmake/3.14.3 ohpc prun/1.3 charliecloud/0.9.7 gnu8/8.3.0 papi/5.7.0 singularity/3.2.1 2.2 Loading modules Modules can be loaded via module load &lt;module&gt;. First, you need to load a C compiler as most programs depend on it. This module is the only one that you should which uses gcc-4.8.5. Make sure to also put this one into your ~/.bashrc file! module load gcc-9.2.0-gcc-4.8.5-o3uv2jk Now you can (for example) load GDAL: module load gdal-3.0.1-gcc-9.2.0-2rbxot4 You can check that it worked via gdalinfo --version 2.3 Checking loaded modules module list 2.4 Default modules It is useful to have some modules loaded by default when logging in to the server. Simply add them to ~/.bashrc. When running R for spatial analysis, the following are required: module load gdal-3.0.1-gcc-9.2.0-2rbxot4 module load geos-3.7.2-gcc-9.2.0-hn6gflr module load proj-5.2.0-gcc-9.2.0-az6mkj5 module load udunits2-2.2.24-gcc-9.2.0-lbs7534 module load pandoc-2.7.3-gcc-9.2.0-tlzhs7d module load openblas-0.3.7-gcc-9.2.0-gwikytn module load zlib-1.2.11-gcc-9.2.0-yuybfrv Furthermore, I recommend the following ones (don’t forget to append your version and compiler): module load byobu-5.127-gcc-9.2.0-x5e5mnz module load git-2.21.0-gcc-9.2.0-hhkbkhg module load ccache-3.3.4-gcc-9.2.0-7klqklq You might wonder why you shouldn’t add a R version to this list? The answer is that it will conflict with RStudio-Server. Whenever opening a session, your ~/.bashrc file is sourced. If you specify an R version there, it will always override what is specified in the RStudio-Server settings related to R-versions. So yes, you always need to specifically load R when logging into a new session. Also make sure that GDAL and friends are loaded in your .bashrc file as otherwise RStudio Server will not find them. If you need to load specifc modules for a project, try to create a wrapper for your project. 2.5 R All versions from v3.4.4 upwards are installed. R is not managed via env modules currently as it causes some problems with RStudio Server. R versions are installed at /opt/R/&lt;version&gt;. To load a specific R version, call the following from the terminal export R_HOME=/opt/R/&lt;version&gt;/lib64/R and then execute R (or preferably radian). You might find the following aliases helpful which you can put into ~/.bashrc: alias r344=&quot;export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.4.4-rzhfe5glzycgamye6qlrvsnetronftpw/rlib/R &amp;&amp; radian&quot; alias r350=&quot;export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.5.0-qpdg7ahjfznh5ivoj4eetzbr3bf4cad7/rlib/R &amp;&amp; radian&quot; alias r351=&quot;export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.5.1-rjchaiabtobk76x2th2ukxsstyk2lnoo/rlib/R &amp;&amp; radian&quot; alias r352=&quot;export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.5.2-ikmvwxzm4redzk5rhubfjjwzkoaskfbl/rlib/R &amp;&amp; radian&quot; alias r353=&quot;export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.5.3-g6smxppqwljnukmag23jfixrd5vabxcd/rlib/R &amp;&amp; radian&quot; alias r360=&quot;export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.6.0-fn2ixao7s2mrvweuhhkceptmew6pgqra/rlib/R &amp;&amp; radian&quot; alias r361=&quot;export R_HOME=/opt/spack/opt/spack/linux-centos7-x86_64/gcc-9.2.0/r-3.6.1-cfrumokg7m7jx4acgu7pwg5suhcngj35/rlib/R &amp;&amp; radian&quot; This enables you to launch the specific R version using the set command, e.g. r360 will launch R v3.6.0. 2.6 RStudio Server RStudio Server Pro is running on port 8787 (https://edi.geogr.uni-jena.de:8787). 2.7 Shiny Server Shiny server is running on port 3838 with user support. This means users can place apps in $HOME/Shinyapps/ and they will be deployed as http://edi.geogr.uni-jena.de:3838/&lt;username&gt;/&lt;appname&gt;. Note the http (https is only available in the Pro version). Exemplary apps: http://edi.geogr.uni-jena.de:3838/patrick/hyperspectral/ http://edi.geogr.uni-jena.de:3838/jannes/cluster_map/ 2.8 Libraries If you cannot find a module you want to use, check it Spack supports it via spack list &lt;module&gt; and contact an admin to install it for you. 2.8.1 Docker Docker is installed on the frontend only (for example to convert .html to .pdf). To use docker, your user needs to be added to the “docker” group. Please contact an Admin in case you want to use it. 2.8.2 OSRM Open Source Routing Machine is installed. Start the server with osrm-routed. More information can be found here. 2.8.3 GDAL Check module avail and look for GDAL to see which versions are available. 2.9 Getting Started If you worked through all these steps, you are almost good to go. Here are have some further recommendations to make your life easier. 2.9.0.1 byobu byobu is a wrapper for your SSH session that makes it possible to close your terminal session and not loose the command running in it. If you start long running jobs, you can safely start them in a byobu window without worrying about them to quit once you shut down your machine. Run byobu after being logged in and a byobu session will be launched. You can open multiple ones with byobu -S &lt;session name&gt;, e.g. byobu -S session2. Once you have multiple ones open, an interactive prompt will ask you which one you want to start next time. 2.9.0.2 radian radian is an optimized R command line tool. You will notice the benefits compared to the default R once you start using it. You need to install it via pip which is already installed if you installed python. Usually setuptools needs to be upgraded first. pip install --user --upgrade setuptools pip install --user radian Now you can either always use radian or set an alias in your .bashrc , e.g. alias r=&quot;radian&quot;. Note that radian only works if you have set the env variable R_HOME correctly. See here for more details. If is does not work at this moment, you might need to add the binary to your $PATH variable in your ~/.bashrc. export PATH=~/.local/bin:$PATH 2.9.0.3 ccache If you load ccache, you will speed-up source installations of R packages a lot. (On Linux, all R packages are installed from source.) Besides loading ccache, you also need to create the following file in your home directory (~/.R/Makevars): (Note that you need to create the folder first, it does not exist by default (mkdir ~/.R/).) CXX_STD = CXX14 VER= CCACHE=ccache CC=$(CCACHE) gcc $(VER) CXX=$(CCACHE) g++$(VER) C11=$(CCACHE) g++$(VER) C14=$(CCACHE) g++$(VER) FC=$(CCACHE) gfortran$(VER) F77=$(CCACHE) gfortran$(VER) When installing a package now, you will occasionally see that the gcc lines are prefixed with ccache. This means that this gcc call was already executed once and is now loaded from the cache rather than being run again. This saves a lot of time, especially for packages that take long to install (dplyr, Rcpp, stringi). 2.9.0.4 Create a wrapper for your project Usually you might want to use a packrat library in a specific directory with a specific R version. Rather than navigating there all the time by hand and loading the R version manually, you can create an alias that does this for you. You can of course also use this approach without packrat - just to load a specific version. alias my-project=&quot;cd /path/to/project &amp;&amp; &lt;load custom env module&gt; &amp;&amp; r361&quot; Put this in your .bashrc and save time :) "],
["scheduler.html", "Chapter 3 Scheduler 3.1 First steps 3.2 SLURM commands", " Chapter 3 Scheduler SLURM is responsible for executing jobs across the cluster. In contrast to running a “direct” R job from the command line on a server without a scheduler, this application takes care of the execution queue while accounting for other running processes. In R, this is done via the R packages rzmq and clustermq. Both packages provide interfaces for the zmq library which in fact is doing the work of sending code requests of any language to SLURM. While the rzmq package is the technical interface to zmq in this combination, the clustermq package is used by the user to start the pipeline of submitting a job to the scheduler. Alternatives to the packages mentioned above are batchtools and future.batchtools. 3.1 First steps Rather than calling a R script directly, you need to wrap your code into a function and invoke it using clustermq::Q(). At first this might seem to be a more complex way to run R code than the interactive line-by-line approach at the command line. But with time you will value the benefits of this approach: By being forced to “functionize” your code and putting in into smaller pieces, you’ll diverge from long R scripts In combination with drake you are able to reproducibly track already built targets and parallelize the building of intermediate targets drake provides an interface to clustermq giving you the possibility of running multiple “targets” (which you can think of as one R script or one intermediate step) in parallel on the HPC. These “targets” can again be parallelized. For more information see the drake manual and especially the section about HPC computing. Is is highly recommended that you set up an SSH key (in case you have not done that already) for passwordless log in. Even thouhg drake is highly recommend, if you do not want to use it, you can also use clustermq::Q() directly. And no, there is no other way to submit your jobs to the compute nodes of the cluster than by using any of the tools mentioned above. See also the section best practice for more information on how to get started. 3.2 SLURM commands While the execution of jobs is explained in more detail in Chapter 4, the following section aims familiarizing yourself with the usage of the scheduler. The basic SLURM commands are sinfo: An overview of the current state of the nodes sinfo PARTITION AVAIL TIMELIMIT NODES STATE NODELIST all* up infinite 4 alloc c[0-2],edi all* up infinite 2 idle c[3-4] frontend up infinite 1 alloc edi threadripper up infinite 4 alloc c[0-2],edi opteron up infinite 2 idle c[3-4] squeue: An overview of the current jobs that are queued, including information about running jobs squeue JOBID PARTITION NAME USER ST TIME NODES NODELIST(REASON) 129_[2-5] threadripper cmq7381 patrick PD 0:00 1 (Resources) 121_2 threadripper cmq7094 patrick R 6:24:17 1 c1 121_3 threadripper cmq7094 patrick R 6:24:17 1 c2 129_1 threadripper cmq7381 patrick R 5:40:44 1 c0 sacct: Overview of jobs that were submitted in the past including their end state 122 cmq7094 threadripper (null) 0 COMPLETED 0:0 123 cmq7094 threadripper (null) 0 PENDING 0:0 121 cmq7094 threadripper (null) 0 PENDING 0:0 125 cmq6623 threadripper (null) 0 FAILED 1:0 126 cmq6623 threadripper (null) 0 FAILED 1:0 127 cmq6623 threadripper (null) 0 FAILED 1:0 128 cmq6623 threadripper (null) 0 FAILED 1:0 124 cmq6623 threadripper (null) 0 FAILED 1:0 130 cmq7381 threadripper (null) 0 PENDING 0:0 scancel: Cancel running jobs using the job ID identifier "],
["submit-jobs.html", "Chapter 4 Submitting jobs 4.1 clustermq setup 4.2 The scheduler template 4.3 Allocating resources 4.4 Monitoring progress 4.5 Summary 4.6 Best practice", " Chapter 4 Submitting jobs 4.1 clustermq setup Every job submission is done via clustermq::Q() (either directly or via drake). See the setup instructions in the clustermq package on how to setup the package. First, you need to set some options in your .Rprofile (on the master node or in your project root when you use packrat): options( clustermq.scheduler = &quot;slurm&quot;, clustermq.template = &quot;&lt;/path/to/file/&quot; ) See the package vignette on how to set up the file. Note that you can have multiple .Rprofile files on your system: Your default R interpreter will use the .Rprofile found in the home directory (~/). But you can also save an .Rprofile file in the root directory of a (RStudio) project (which will be preferred over the one in $HOME). This way you can use customized .Rprofile files tailored to a project. At this stage you should be able to run the example at the top of the README of the clustermq package. It is a very simple example which finishes in a few seconds. If it does not work, you either did something wrong or the nodes are busy. Check with sinfo and squeue. Otherwise see the troubleshooting chapter. Be aware of setting n_cpus in the template argument of clustermq::Q() if your submitted job is parallelized! If you submit a job that is parallelized without telling the scheduler, the scheduler will reserve 1 core for this job (because it thinks it is sequential) but in fact multiple processes will spawn. This will potentially affect all running processes on the server since the scheduler will accept more processing than it actually can take. 4.2 The scheduler template To successfully submit jobs to the scheduler, you need to set the .Rprofile options given above. Note that you can add any bash commands into the scripts between the SBATCH section and the final R call. For example, one of my templates looks like this: #!/bin/sh #SBATCH --job-name={{ job_name }} #SBATCH --partition=threadripper #SBATCH --output={{ log_file | /dev/null }} # you can add .%a for array index #SBATCH --error={{ log_file | /dev/null }} #SBATCH --cpus-per-task={{ n_cpus }} #SBATCH --mem={{ memory }} #SBATCH --array=1-{{ n_jobs }} cd path/to/project # load desired R version module load r-3.5.2-gcc-9.2.0-womfaw CMQ_AUTH={{ auth }} R --no-save --no-restore -e &#39;clustermq:::worker(&quot;{{ master }}&quot;)&#39; Note: The # signs are no mistakes here, they are no “comment” signs in this context. The SBATCH commands will be executed here. You can simply copy it and adjust it to your needs (set the right path to your project and specify the R version you want to use). 4.3 Allocating resources There are two approaches/packages you can use: drake (recommended) clustermq drake::make(parallelism = &quot;clustermq&quot;, n_jobs = 1, template = list(n_cpus = &lt;X&gt;, log_file = &lt;Y&gt;, memory = &lt;Z&gt;)) clustermq::Q(template = list(n_cpus = &lt;X&gt;, log_file = &lt;Y&gt;, memory = &lt;Z&gt;)) (The individual components of these calls are explained in more detail below.) Note that drake uses clustermq under the hood. Notations like &lt;X&gt; are meant to be read as placeholders, meaning you need to replace them with valid content.) When submitting jobs via clustermq::Q(), it is important to tell the scheduler how many cores and memory should be reserved for you. This step is very important. If you specify less cores than you actually use in your script (e.g. by internal parallelization), the scheduler will plan with X cores although your submitted code will spawn Y processes in the background. This might overload the node and eventually cause your script (and more importantly) the processes of others to crash. There are two ways to specify these settings, depending on which approach you use: via clustermq::Q() directly Pass the values via argument template like template = list(n_cpus = &lt;X&gt;, memory = &lt;Y&gt;). It will then be passed to the clustermq.template file (frequently named slurm_clustermq.tmpl) which contains following lines: #SBATCH --cpus-per-task{{ n_cpus }} #SBATCH --mem={{ memory }} This tells the scheduler how many resources (here cpus) your job needs. via drake::make() (recommended) Again, set the options via argument template = list(n_cpus = X, memory = Y). See section “The resources column for transient workers” in the drake manual. Please think upfront how many cpus and memory your task requires. The following two examples show you the implications of wrong specifications. mclapply(cores = 20) (in your script) &gt; n_cpus = 16 In this case, four workers will always be in “waiting mode” since only 16 cpus can be used by your resource request. This slows down your parallelization but does no harm to other users. mclapply(cores = 11) &lt; n_cpus = 16 In this case, you reserve 16 CPUs from the machine but only use 11 at most. This blocks five CPUs of the machine for no reason potentially causing other people to be added to the queue rather than getting their job processed immediately. Furthermore, if you want to use all resources of a node and run into memory problems, try reducing the number of CPUs (if you already increased the memory to its maximum). If you scale down the number of CPUs, you will have more memory/cpu available. 4.4 Monitoring progress When submitting jobs you can track its progress by specifying a log_file in the clustermq::Q() call, e.g. clustermq::Q(template = list(log_file = path/to/file)). For drake, the equivalent is to specify console_log_file() in either make() or drake_config(). If your jobs are running on a node, you can SSH into the node, e.g. ssh c0. There you can take a look at the current load by using htop. Note that you can only log in if you have a running progress on a specific node. Another option is to take a look Ganglia to see the load of the HPC. 4.5 Summary Set up your .Rprofile with options(clustermq.template = &quot;/path/to/file&quot;). The clustermq.template should point to a SLURM template file in your $HOME or project directory. Decide which approach you want to use: drake::make(parallelism = &quot;clustermq&quot;, n_jobs = 1, template = list(n_cpus = X, log_file = Y, memory = Z)) (recommended!) clustermq::Q(template = list(n_cpus = X, log_file = Y, memory = Z)) You need to have a Slurm template file in your project directory. This template needs to be linked in your .Rprofile with options(clustermq.template = &quot;/path/to/file&quot;). 4.6 Best practice You can submit as many tasks as you want in separate R sessions, they will be processed as ranked in the queue of the scheduler. However, submitting single jobs that finish quickly is tedious. The same applies to the submission of multiple jobs in separate R sessions. These two points are the reason why it is highly recommended to use drake. By using drake, all of your intermediate R objects are “targets”. You can specify any number of targets that should be build when calling drake::make(). drake will take care that for each target as separate job is created which is then added to the SLURM queue. For example, if you want to build three R objects named object1, object2 and object3 in parallel, one on each node: drake::make(plan, targets = c(&quot;object1&quot;, &quot;object2&quot;, &quot;object3&quot;), jobs = 3, template = list(n_cpus = 16, memory = 320000, log_file = &quot;/path/to/log.txt&quot;)) Creates three jobs which are added to the SLURM queue Each jobs requires 16 cores Each job needs 32 GB of memory The call is finished once all jobs have finished. After they are done, they will be marked as “built” and added to a cache directory by drake, so you cannot accidentally rebuild them (unless you change something substantial in the code). If you follow this practice, the only thing you need to execute on the cluster will be this call (after you initialized the config by sourcing your drake.R file). "],
["troubleshooting-and-cautionary-notes.html", "Chapter 5 Troubleshooting and Cautionary Notes 5.1 Cautionary Notes 5.2 FAQ", " Chapter 5 Troubleshooting and Cautionary Notes 5.1 Cautionary Notes 5.2 FAQ Question: I ran sinfo and saw that the state of the nodes is “down”. Answer: Please contact the admin to resume the nodes. Question: I ran sinfo and saw that some nodes have status “mix” and some “alloc”. What are the differences? Answer: “mix” means that the node is not fully loaded and is in a mixed state between processing and being idle. “alloc” means that the node is fully allocated. Question: I cannot install udunits2. What shall I do? Answer: Install it via install.packages('udunits2', type = 'source', configure.args='--with-udunits2-lib=&lt;path-to-installation&gt;'. For example &quot;--with-udunits2-include=/opt/spack/opt/spack/linux-centos7-zen/gcc-9.2.0/udunits2-2.2.24-lbs7534pnuqex4ce3s3houcgcpabyrhq/include/&quot;. You can set this option permanently in ~/.Rprofile in options(configure.args = list(udunits = c(&lt;&gt;))). Question: I cannot install rgdal | sf. What shall I do? Answer: All the spatial packages are a bit troublesome since they expect certain libraries in predefined directories. If this is not the case, the locations need to be given explicitly during installation. You need to install it with custom options set for configure.args, similar to udunits above. The following works: options( configure.args=list( rgdal = c( &quot;--with-proj-lib=/opt/spack/opt/spack/linux-centos7-zen/gcc-9.2.0/proj-5.2.0-az6mkj55zpnh6fmg2ae5wyrmhfiynxfx/lib&quot;), sf = c( &quot;--with-proj-lib=/opt/spack/opt/spack/linux-centos7-zen/gcc-9.2.0/proj-5.2.0-az6mkj55zpnh6fmg2ae5wyrmhfiynxfx/lib&quot;) ) ) "]
]
